---
title: "AI Agents"
description: "Build fault-tolerant, observable AI agents that recover from failures and support advanced interaction patterns."
icon: "robot"
---

import { GitHubLink } from '/snippets/blocks/github-link.mdx';

AI agents are long-running processes that combine LLMs with tools and external APIs to complete complex tasks. With Restate, you can build agents that are **resilient to failures**, **stateful across conversations**, and **observable** without managing complex retry logic or external state stores.

In this guide, you'll learn how to:
- Build durable AI agents that recover automatically from crashes and API failures
- Integrate with existing AI SDKs like Vercel AI SDK and OpenAI
- Implement resilient human-in-the-loop workflows with approvals and timeouts
- Manage conversation history and state across multi-turn interactions
- Orchestrate multiple agents working together on complex tasks


## Getting Started

A Restate AI application has two main components:
- **Restate Server**: The core engine that manages durable execution and orchestrates your agents
- **Agent Services**: Your agent or AI workflow logic using the Restate SDK with automatic durability and state management

<img src="/img/tour/app_layout.svg" alt="Application Structure"/>

A basic AI agent that uses tools looks like this:

```typescript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/main/vercel-ai/template/src/app.ts?collapse_prequel"} 
export const agent = restate.service({
  name: "assistant",
  handlers: {
    run: async (ctx: Context, { messages }: { messages: CoreMessage[] }) => {
      const { fullStream } = await streamObject({
        model: openai("gpt-4o-mini"),
        messages,
        tools: { getWeather: weatherTool },
        maxSteps: 10,
      }))

      const result = await fullStream;
      return result;
    },
  },
});
```

An agent service has handlers that can be called over HTTP. The agent receives messages and uses the Restate `Context` to make LLM calls and tool executions durable - they automatically recover from failures without losing progress.

You don't need to run your agents in any special way. Restate works with how you already deploy your code, whether that's in Docker, on Kubernetes, or via AWS Lambda.

### Run the example
[Install Restate](/develop/local_dev) and launch it:
```bash
restate-server
```

Get the example:
```bash
restate example typescript-ai-agents && cd typescript-ai-agents
npm install
```
<GitHubLink url="https://github.com/restatedev/ai-examples/tree/main/vercel-ai/template" />

Run the example:
```bash
npm run dev
```

Then, tell Restate where your agent is running via the UI (`http://localhost:9070`) or CLI:
```bash
restate deployments register http://localhost:9080
```

To invoke the agent, send a request to `restate-ingress/ServiceName/handlerName`:
```bash
curl localhost:8080/assistant/run \
  --json '{"messages": [{"role": "user", "content": "What is the weather in San Francisco?"}]}'
```

Click in the UI's invocations tab to see the execution trace showing the LLM call and weather tool execution.

<Frame>
<img src="/img/tour/ai-agents/basic-trace.png" alt="Agent execution trace" />
</Frame>


## Durable Execution

AI agents make multiple LLM calls and tool executions that can fail due to rate limits, network issues, or service outages. Restate uses Durable Execution to ensure your agents recover their progress without starting over.

Whenever an agent executes an operation with the Restate `Context`, this gets sent to the Restate Server and persisted in a log. On a failure or crash, Restate automatically retries with the log of completed operations, allowing the agent to resume from where it left off.

<img src="/img/tour/durable_execution_ai.gif" alt="Durable AI Agent Execution" />

Here's an agent that performs multiple steps to analyze and respond to user queries:

```typescript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/main/vercel-ai/examples/restate/services/multi_tool.ts?collapse_prequel"} 
export const calculatorAgent = restate.service({
  name: "calculator-agent",
  handlers: {
    run: async (ctx: Context, request: CalculatorRequest) => {
      // Each step is durable and recoverable
      const analysis = await ctx.run("analyze", async () => {
        return await generateText({
          model: openai("gpt-4o-mini"),
          prompt: `Analyze this math problem: ${request.problem}`,
        });
      });

      const calculation = await ctx.run("calculate", async () => {
        return await generateObject({
          model: openai("gpt-4o-mini"),
          schema: calculatorSchema,
          tools: { calculate: calculatorTool },
          messages: [{ role: "user", content: request.problem }],
        });
      });

      const response = await ctx.run("respond", async () => {
        return await generateText({
          model: openai("gpt-4o-mini"),
          prompt: `Explain this result: ${calculation.object.result}`,
        });
      });

      return { analysis: analysis.text, calculation: calculation.object, response: response.text };
    },
  },
});
```

**Key Benefits:**
- `ctx.run()` makes LLM calls and tool executions durable - they replay results on failures
- If the agent crashes after analysis, it resumes at the calculation step
- No duplicate API calls or lost progress
- Full execution traces for debugging and monitoring

<Accordion title={"Try it out"}>
    Try solving a complex math problem:
    ```bash
    curl localhost:8080/calculator-agent/run \
      --json '{"problem": "What is the compound interest on $1000 at 5% annually for 3 years?"}'
    ```

    In the invocation page, you can see each step being executed durably. Try restarting the service during execution to see how it recovers seamlessly.
</Accordion>

> **Compared to workflow engines:** Restate's durable function model allows you to keep the entire agent as one long-running function. You don't need to split LLM calls and tool executions into separate activities, keeping context and shared resources together.


## Agent Loops with AI SDKs

Real AI agents often run in loops, making multiple LLM calls and tool executions until they complete a task. Restate integrates seamlessly with popular AI SDKs to make these loops fault-tolerant.

Here's how to build an agent loop using the Vercel AI SDK:

```typescript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/main/vercel-ai/examples/restate/services/multi_tool.ts"} 
export const mathAgent = restate.service({
  name: "math-agent",
  handlers: {
    solve: async (ctx: Context, { problem }: { problem: string }) => {
      let step = 0;
      const maxSteps = 10;
      const messages: CoreMessage[] = [
        { role: "system", content: "You are a helpful math tutor. Solve problems step by step." },
        { role: "user", content: problem }
      ];

      while (step < maxSteps) {
        const result = await ctx.run(`step-${step}`, async () => {
          return await streamObject({
            model: openai("gpt-4o-mini"),
            messages,
            tools: {
              calculate: {
                description: "Calculate mathematical expressions",
                parameters: z.object({
                  expression: z.string(),
                }),
                execute: async ({ expression }) => {
                  const result = evaluate(expression);
                  return { result };
                },
              },
            },
            maxSteps: 1,
          });
        });

        // Process the result and add to conversation
        const response = await result.fullStream;
        
        if (response.finishReason === "stop") {
          return response.object;
        }

        messages.push(...response.messages);
        step++;
      }

      return { error: "Max steps reached" };
    },
  },
});
```

**Integration Benefits:**
- Works with any AI SDK (Vercel AI, OpenAI, Anthropic, etc.)
- Each loop iteration is durable and recoverable
- Automatic retry on API failures with exponential backoff
- Tool executions are cached and don't re-execute on replay
- Conversation state persists across crashes

<Accordion title={"Try it out"}>
    Send a complex math problem that requires multiple steps:
    ```bash
    curl localhost:8080/math-agent/solve \
      --json '{"problem": "A store offers a 20% discount, then adds 8% tax. If I buy items worth $150, how much do I pay?"}'
    ```

    Watch in the UI how the agent makes multiple LLM calls and tool executions, with each step being recorded durably.
</Accordion>



## Observability and Debugging

Restate provides built-in observability for AI agents, giving you complete visibility into LLM calls, tool executions, and conversation flows without additional instrumentation.

Every agent execution creates a detailed trace showing:
- Individual LLM calls with prompts and responses
- Tool executions with inputs and outputs  
- Timing and retry information
- State changes and conversation history
- Error details and recovery attempts

<Frame>
<img src="/img/tour/ai-agents/agent-trace.png" alt="AI Agent Execution Trace" />
</Frame>

Click on any invocation in the UI to see the complete execution timeline. This makes debugging agent behavior, understanding decision paths, and optimizing performance straightforward.

<Accordion title={"Crash Recovery in Action"}>
    AI agents can crash during any step - while waiting for an LLM response, during tool execution, or while processing results. Restate automatically recovers the agent to exactly where it left off.

    Try this:
    1. Start a long-running agent task
    2. Kill the process while it's executing  
    3. Restart the service
    4. Watch in the UI as execution resumes from the exact step where it crashed

    <Frame>
    <img src="/img/tour/ai-agents/crash-recovery.png" alt="Agent Crash Recovery" />
    </Frame>
</Accordion>

<Accordion title={"OpenTelemetry Integration"}>
    Restate supports OpenTelemetry for exporting traces to external systems like Langfuse, DataDog, or Jaeger:

    ```bash
    # Set environment variables for OTEL export
    export OTEL_EXPORTER_OTLP_ENDPOINT=https://your-langfuse-instance.com/api/public/ingestion
    export OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer your-api-key"
    
    # Start Restate with OTEL enabled
    restate-server --otel-endpoint $OTEL_EXPORTER_OTLP_ENDPOINT
    ```

    This gives you AI agent traces alongside your existing observability stack.
</Accordion>


## Human-in-the-Loop Workflows

Many AI agents need human oversight for high-risk decisions, compliance requirements, or handling edge cases. Restate makes it easy to pause agent execution, wait for human input, and handle timeouts gracefully.

### Workflow Approval Pattern

Here's a loan processing agent that requires human approval for uncertain cases:

```typescript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/main/vercel-ai/examples/restate/services/human_approval.ts?collapse_prequel"} 
export const loanProcessor = restate.workflow({
  name: "loan-processor",
  handlers: {
    process: async (ctx: WorkflowContext, application: LoanApplication) => {
      const loanId = ctx.key;

      // AI assessment step
      const assessment = await ctx.run("assess", async () => {
        return await generateObject({
          model: openai("gpt-4o-mini"),
          schema: assessmentSchema,
          messages: [{
            role: "user",
            content: `Assess this loan application: ${JSON.stringify(application)}`
          }],
        });
      });

      // Automatic approval for high-confidence cases
      if (assessment.confidence > 0.9 && assessment.recommendation === "approve") {
        return { approved: true, reason: "Automatic approval - high confidence" };
      }

      // Require human approval for uncertain cases
      const humanDecision = await ctx.promise<ApprovalDecision>("human-approval");
      
      return {
        approved: humanDecision.approved,
        reason: humanDecision.reason,
        approver: humanDecision.approver
      };
    },

    // Human approver calls this endpoint
    approve: async (ctx: WorkflowSharedContext, decision: ApprovalDecision) => {
      ctx.promiseResolve("human-approval", decision);
    },
  },
});
```

### Tool Approval Pattern

For agents that need approval for specific tool calls, you can implement approval within the agent loop:

```typescript
// Within an agent loop
if (toolCall.name === "transfer_money" && toolCall.amount > 10000) {
  // Request approval for high-value transfers
  const approval = await ctx.promise<boolean>("approval-" + toolCall.id);
  if (!approval) {
    continue; // Skip this tool call
  }
}

// Execute the approved tool
const result = await ctx.run(`tool-${toolCall.id}`, () => 
  executeTool(toolCall)
);
```

**Benefits with Restate:**
- Agents pause indefinitely while waiting for human input
- No timeouts or lost context during approval workflows
- Full audit trail of AI decisions and human overrides
- Agents resume exactly where they left off after approval

<Accordion title={"Try it out"}>
    Process a loan application that requires human approval:
    ```bash
    curl localhost:8080/loan-processor/loan-123/process \
      --json '{
        "applicant": "John Doe",
        "amount": 50000,
        "creditScore": 650,
        "income": 75000
      }'
    ```

    The workflow will pause waiting for human approval. Provide approval:
    ```bash
    curl localhost:8080/loan-processor/loan-123/approve \
      --json '{
        "approved": true,
        "reason": "Good income-to-debt ratio",
        "approver": "sarah@bank.com"
      }'
    ```

    Watch in the UI how the workflow pauses and resumes after human input.
</Accordion>

<Accordion title={"Timeouts and Escalation"}>
    Add timeouts to human approval steps to prevent workflows from hanging indefinitely:

    ```typescript
    try {
      const decision = await ctx.promise<ApprovalDecision>("human-approval")
        .orTimeout({ hours: 24 });
      return decision;
    } catch (e) {
      if (e instanceof TimeoutError) {
        // Escalate to manager or auto-reject
        return { approved: false, reason: "Approval timeout - escalated" };
      }
      throw e;
    }
    ```
</Accordion>



## Multi-turn Conversations and State

AI agents often need to maintain conversation history and context across multiple interactions. Restate's Virtual Objects provide stateful, isolated contexts for each user or conversation thread.

Here's a stateful chat agent that remembers conversation history:

```typescript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/main/vercel-ai/examples/restate/services/chat.ts?collapse_prequel"} 
export const conversationAgent = restate.object({
  name: "conversation",
  handlers: {
    chat: async (ctx: ObjectContext, { message }: { message: string }) => {
      const conversationId = ctx.key; // Unique per conversation

      // Retrieve existing conversation state
      let state = await ctx.get<ChatState>("state") ?? {
        messages: [],
        context: {},
        createdAt: Date.now()
      };

      // Add user message to history
      state.messages.push({ role: "user", content: message });

      // Generate AI response with full conversation history
      const response = await ctx.run("generate", async () => {
        return await generateText({
          model: openai("gpt-4o-mini"),
          messages: [
            { role: "system", content: "You are a helpful assistant. Remember previous conversation context." },
            ...state.messages
          ],
        });
      });

      // Add AI response to history and save state
      state.messages.push({ role: "assistant", content: response.text });
      state.context.lastInteraction = Date.now();
      
      ctx.set("state", state);

      return {
        response: response.text,
        conversationId,
        messageCount: state.messages.length
      };
    },

    // Get conversation history
    getHistory: async (ctx: ObjectSharedContext) => {
      const state = await ctx.get<ChatState>("state");
      return state?.messages ?? [];
    },

    // Clear conversation
    reset: async (ctx: ObjectContext) => {
      ctx.clear("state");
      return { reset: true };
    },
  },
});
```

**State Management Benefits:**
- Each conversation gets isolated, persistent state
- Conversation history survives service restarts and crashes
- No external database required - state is automatically managed
- Concurrent conversations are processed safely without race conditions
- State is queryable and exportable via the UI

<Frame>
<img src="/img/tour/ai-agents/conversation-state.png" alt="Conversation State Management" />
</Frame>

<Accordion title={"Try it out"}>
    Start a conversation with a specific ID:
    ```bash
    curl localhost:8080/conversation/user-123/chat \
      --json '{"message": "Hi, I am planning a trip to Japan. What should I know?"}'
    ```

    Continue the conversation - the agent remembers previous context:
    ```bash
    curl localhost:8080/conversation/user-123/chat \
      --json '{"message": "What about the best time to visit?"}'
    ```

    Get conversation history:
    ```bash
    curl localhost:8080/conversation/user-123/getHistory
    ```

    The agent maintains context across all interactions within the same conversation ID.
</Accordion>



## Concurrent Tasks and Sub-agents

Complex AI workflows often benefit from parallel execution - running multiple agents simultaneously or breaking work into concurrent sub-tasks. This reduces overall execution time and allows specialized agents to work in parallel.

Here's an agent that parallelizes research tasks:

```typescript
export const researchOrchestrator = restate.service({
  name: "research-orchestrator",
  handlers: {
    research: async (ctx: Context, { topic, aspects }: ResearchRequest) => {
      // Start multiple research agents in parallel
      const researchPromises = aspects.map(aspect => 
        ctx.run(`research-${aspect}`, async () => {
          return await generateObject({
            model: openai("gpt-4o-mini"),
            schema: researchSchema,
            messages: [{
              role: "user",
              content: `Research ${aspect} of ${topic}. Provide detailed analysis.`
            }],
            tools: { webSearch: searchTool, getDocument: documentTool }
          });
        })
      );

      // Wait for all research to complete
      const results = await Promise.all(researchPromises);

      // Synthesize findings
      const synthesis = await ctx.run("synthesize", async () => {
        return await generateText({
          model: openai("gpt-4o-mini"),
          messages: [{
            role: "user",
            content: `Synthesize these research findings into a comprehensive report: ${JSON.stringify(results)}`
          }]
        });
      });

      return {
        topic,
        individualFindings: results,
        synthesis: synthesis.text,
        completedAt: new Date().toISOString()
      };
    },
  },
});
```

**Concurrency Benefits:**
- Multiple agents or tasks run simultaneously
- Shorter recovery time (replay only failed parts)
- Reduced memory usage (shorter execution logs)
- Better resource utilization
- Fault isolation (one task failure doesn't affect others)

<Accordion title={"Sub-workflow Pattern"}>
    For complex workflows, break them into separate durable functions:

    ```typescript
    // Main orchestrator
    export const mainWorkflow = restate.service({
      name: "main-workflow",
      handlers: {
        process: async (ctx: Context, input: ProcessInput) => {
          // Delegate to specialized sub-workflows
          const [analysis, validation, enrichment] = await Promise.all([
            ctx.serviceClient(analysisWorkflow).analyze(input.data),
            ctx.serviceClient(validationWorkflow).validate(input.rules),
            ctx.serviceClient(enrichmentWorkflow).enrich(input.context)
          ]);

          return { analysis, validation, enrichment };
        },
      },
    });
    ```

    This pattern provides:
    - **Simpler recovery**: Each sub-workflow recovers independently  
    - **Better observability**: Clear separation of concerns in traces
    - **Scalability**: Sub-workflows can scale independently
    - **Reusability**: Sub-workflows can be used by multiple orchestrators
</Accordion>


## Multi-Agent Orchestration

Multi-agent systems coordinate specialized agents to solve complex problems. Restate provides reliable orchestration patterns where agents can communicate durably and maintain shared context.

Here's a loan evaluation system with multiple specialized agents:

```typescript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/main/vercel-ai/examples/restate/services/multi_agent.ts?collapse_prequel"} 
// Risk assessment specialist
export const riskAgent = restate.service({
  name: "risk-agent",
  handlers: {
    assess: async (ctx: Context, application: LoanApplication) => {
      return await ctx.run("risk-analysis", async () => {
        return await generateObject({
          model: openai("gpt-4o-mini"),
          schema: riskAssessmentSchema,
          messages: [{
            role: "user",
            content: `Assess the risk for this loan application: ${JSON.stringify(application)}`
          }],
          tools: { checkCreditHistory: creditTool, analyzeIncome: incomeTool }
        });
      });
    },
  },
});

// Main orchestrator that coordinates multiple agents
export const loanEvaluator = restate.workflow({
  name: "loan-evaluator",
  handlers: {
    evaluate: async (ctx: WorkflowContext, application: LoanApplication) => {
      const loanId = ctx.key;

      // Coordinate multiple agents in parallel
      const [riskAssessment, creditCheck, incomeVerification] = await Promise.all([
        ctx.serviceClient(riskAgent).assess(application),
        ctx.serviceClient(creditAgent).verify(application.ssn),
        ctx.serviceClient(incomeAgent).verify(application.income)
      ]);

      // Final decision with all agent inputs
      const decision = await ctx.run("decide", async () => {
        return await generateObject({
          model: openai("gpt-4o-mini"),
          schema: loanDecisionSchema,
          messages: [{
            role: "user",
            content: `Make a loan decision based on:\nRisk: ${JSON.stringify(riskAssessment)}\nCredit: ${JSON.stringify(creditCheck)}\nIncome: ${JSON.stringify(incomeVerification)}`
          }]
        });
      });

      return {
        loanId,
        decision: decision.object,
        assessments: { riskAssessment, creditCheck, incomeVerification },
        processedAt: new Date().toISOString()
      };
    },
  },
});
```

### Stateful Agent Coordination

For complex multi-agent workflows, use a Virtual Object as the orchestrator to maintain shared context:

```typescript
export const projectOrchestrator = restate.object({
  name: "project-orchestrator",
  handlers: {
    start: async (ctx: ObjectContext, { projectSpec }: ProjectRequest) => {
      const projectId = ctx.key;
      
      // Initialize shared state
      await ctx.set("project", {
        spec: projectSpec,
        status: "planning",
        agents: [],
        results: {},
        startedAt: Date.now()
      });

      // Assign work to different agents
      const assignments = await ctx.run("plan", () => planProject(projectSpec));
      
      // Track agent assignments
      for (const assignment of assignments) {
        ctx.serviceSendClient(assignmentQueue).assign({
          projectId,
          agentType: assignment.type,
          task: assignment.task
        });
      }

      return { projectId, assignments };
    },

    // Agents call this to report progress
    updateProgress: async (ctx: ObjectContext, update: ProgressUpdate) => {
      const project = await ctx.get<Project>("project");
      project.results[update.agentId] = update.result;
      
      // Check if all agents completed
      if (this.allAgentsComplete(project)) {
        project.status = "completed";
        // Trigger final synthesis
        ctx.serviceSendClient(synthesisAgent).synthesize(project.results);
      }
      
      await ctx.set("project", project);
    },
  },
});
```

**Multi-Agent Benefits:**
- **Specialized expertise**: Each agent focuses on specific domains
- **Fault tolerance**: Agent failures don't crash the entire system  
- **Scalability**: Agents can be scaled independently
- **Observability**: Clear visibility into inter-agent communication
- **Coordination**: Shared state and reliable message passing

<Accordion title={"Try it out"}>
    Start a loan evaluation with multiple agents:
    ```bash
    curl localhost:8080/loan-evaluator/loan-456/evaluate \
      --json '{
        "applicant": "Jane Smith",
        "amount": 75000,
        "ssn": "123-45-6789",
        "income": 90000,
        "purpose": "home improvement"
      }'
    ```

    Watch in the UI how the orchestrator coordinates multiple specialized agents in parallel, each contributing their expertise to the final decision.
</Accordion>


## Advanced Patterns

Restate enables sophisticated AI agent patterns that would be complex to implement with traditional architectures. Here are some advanced patterns for production AI systems:

### Real-time Progress Updates

Stream agent progress to users in real-time using Restate's pub/sub capabilities:

```typescript
export const streamingAgent = restate.service({
  name: "streaming-agent",
  handlers: {
    process: async (ctx: Context, { userId, task }: StreamingRequest) => {
      // Publish progress updates as the agent works
      const publishProgress = (step: string, data: any) => {
        ctx.serviceSendClient(notificationService).notify({
          userId,
          type: "agent-progress",
          step,
          data,
          timestamp: Date.now()
        });
      };

      publishProgress("started", { task });

      const analysis = await ctx.run("analyze", async () => {
        const result = await analyzeTask(task);
        publishProgress("analysis-complete", result);
        return result;
      });

      const execution = await ctx.run("execute", async () => {
        const result = await executeTask(analysis);
        publishProgress("execution-complete", result);
        return result;
      });

      publishProgress("finished", { analysis, execution });
      return { analysis, execution };
    },
  },
});
```

### Interruptible Agents

Build agents that can be interrupted and redirected mid-execution:

```typescript
export const codingAgent = restate.object({
  name: "coding-agent",
  handlers: {
    code: async (ctx: ObjectContext, { prompt }: CodingRequest) => {
      const sessionId = ctx.key;
      let currentStep = 0;
      
      while (currentStep < 10) {
        // Check for interruption signals
        const interruption = await ctx.get<Interruption>("interruption");
        if (interruption) {
          ctx.clear("interruption");
          // Handle the interruption
          return await this.handleInterruption(ctx, interruption);
        }

        // Normal agent step
        const result = await ctx.run(`step-${currentStep}`, async () => {
          return await generateCode(prompt, currentStep);
        });

        if (result.completed) break;
        currentStep++;
      }

      return { code: result.code, steps: currentStep };
    },

    // External systems can interrupt the agent
    interrupt: async (ctx: ObjectContext, interruption: Interruption) => {
      await ctx.set("interruption", interruption);
      return { interrupted: true };
    },
  },
});
```

### Long-running Agent Workflows

Build agents that can run for days or weeks, handling external events and timeouts:

```typescript
export const campaignAgent = restate.workflow({
  name: "campaign-agent",
  handlers: {
    run: async (ctx: WorkflowContext, campaign: Campaign) => {
      const campaignId = ctx.key;
      
      // Run campaign for specified duration
      const endTime = Date.now() + campaign.durationMs;
      
      while (Date.now() < endTime) {
        // Wait for external events or timeout
        try {
          const event = await ctx.promise<CampaignEvent>("event")
            .orTimeout({ hours: 6 }); // Check every 6 hours
            
          await this.handleEvent(ctx, event);
        } catch (e) {
          if (e instanceof TimeoutError) {
            // Regular campaign maintenance
            await ctx.run("maintenance", () => 
              performCampaignMaintenance(campaignId)
            );
          }
        }
      }

      return { campaignId, status: "completed" };
    },

    event: async (ctx: WorkflowSharedContext, event: CampaignEvent) => {
      ctx.promiseResolve("event", event);
    },
  },
});
```

### Integration with External AI Platforms

Integrate with external AI platforms like Langfuse for advanced monitoring:

```typescript
// Export traces to Langfuse
export const observableAgent = restate.service({
  name: "observable-agent",
  handlers: {
    process: async (ctx: Context, input: ProcessInput) => {
      // Start Langfuse trace
      const trace = await ctx.run("start-trace", () => 
        langfuse.trace({
          name: "agent-process",
          input,
          metadata: { restateInvocationId: ctx.invocationId }
        })
      );

      try {
        const result = await ctx.run("llm-call", async () => {
          const generation = trace.generation({ name: "main-llm-call" });
          const result = await openai.chat.completions.create({...});
          generation.end({ output: result });
          return result;
        });

        trace.update({ output: result });
        return result;
      } catch (error) {
        trace.update({ output: error });
        throw error;
      }
    },
  },
});
```

These patterns enable building production-grade AI systems with:
- **Real-time user feedback** during long-running agent processes
- **Interruptible execution** for dynamic agent behavior  
- **Long-running workflows** that survive service restarts
- **Advanced observability** with external AI monitoring tools
- **Event-driven architecture** for responsive AI systems

## Summary

Restate transforms AI agent development by providing:

- **Durable Execution**: Automatic recovery from failures without losing progress
- **State Management**: Persistent conversation history and context
- **SDK Integration**: Works with popular AI SDKs like Vercel AI and OpenAI
- **Human-in-the-Loop**: Seamless approval workflows with timeouts  
- **Multi-Agent Coordination**: Reliable orchestration of specialized agents
- **Advanced Patterns**: Real-time streaming, interruptions, and long-running workflows

Build resilient, observable AI agents that handle real-world complexity without the typical infrastructure overhead.
