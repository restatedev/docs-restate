---
title: "AI Agents"
description: "Build fault-tolerant, observable AI agents that recover from failures and support advanced interaction patterns."
icon: "robot"
---

import { GitHubLink } from '/snippets/blocks/github-link.mdx';

AI agents are long-running processes that combine LLMs with tools and external APIs to complete complex tasks. With Restate, you can build agents that are **resilient to failures**, **stateful across conversations**, and **observable** without managing complex retry logic or external state stores.

In this guide, you'll learn how to:
- Build durable AI agents that recover automatically from crashes and API failures
- Integrate with existing AI SDKs like Vercel AI SDK and OpenAI
- Observe and debug agent executions with detailed traces
- Implement resilient human-in-the-loop workflows with approvals and timeouts
- Manage conversation history and state across multi-turn interactions
- Orchestrate multiple agents working together on complex tasks

## Getting Started

A Restate AI application has two main components:
- **Restate Server**: The core engine that takes care of the orchestration and resiliency of your agents
- **Agent Services**: Your agent or AI workflow logic using the Restate SDK for durability

<img src="/img/tour/app_layout.svg" alt="Application Structure"/>

Restate works with how you already deploy your agents, whether that's in Docker, on Kubernetes, or via AWS Lambda. You don't need to run your agents in any special way.

Let's run an example locally to get a better feel for how it works.

### Run the example
[Install Restate](/develop/local_dev) and launch it:
```bash
restate-server
```

Get the example:
```bash
restate example typescript-ai-agents && cd typescript-ai-agents
npm install
```
<GitHubLink url="https://github.com/restatedev/ai-examples" />

Run the agent:
```bash
npm run dev
```

Then, tell Restate where your agent is running via the UI (`http://localhost:9070`) or CLI:
```bash
restate deployments register http://localhost:9080
```

This registers a set of agents which we will be covering in this tutorial.

To test your setup, invoke the weather agent, either via the UI playground (by clicking on the service) or CLI:
```bash
curl localhost:8080/WeatherAgent/run \
  --json '{"prompt": "What is the weather like in San Francisco?"}'
```

You should see the weather information printed in the terminal.

Let's have a look at what happened under the hood.

## Durable Execution

AI agents make multiple LLM calls and tool executions that can fail due to rate limits, network issues, or service outages.
Restate uses Durable Execution to make your agents withstand failures without losing progress.

The Restate SDK records the steps the agent executes in a log and replays them if the process crashes or is restarted:
<img src="/img/tour/durable_execution_microservices.gif" alt="Durable AI Agent Execution" />

### Implementing a Durable Agent
To implement a durable agent, you can use the Restate SDK in combination with existing AI frameworks like the Vercel AI SDK.

Here's the implementation of the durable weather agent you just invoked:

```typescript getstarted/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/getstarted/agent.ts?collapse_prequel"} 
export default restate.service({
  name: "WeatherAgent",
  handlers: {
    run: async (ctx: restate.Context, { prompt }: { prompt: string }) => {
      const model = wrapLanguageModel({
        model: openai("gpt-4o"),
        middleware: durableCalls(ctx, { maxRetryAttempts: 3 }),
      });

      const { text } = await generateText({
        model,
        system: "You are a helpful agent that provides weather updates.",
        prompt,
        tools: {
          getWeather: tool({
            description: "Get the current weather for a given city.",
            inputSchema: z.object({ city: z.string() }),
            execute: async ({ city }) =>
              ctx.run("get weather", () => fetchWeather(city)),
          }),
        },
        stopWhen: [stepCountIs(5)],
        providerOptions: { openai: { parallelToolCalls: false } },
      });

      return text;
    },
  },
});
```

The agent logic is implemented in a handler of a Restate service, here the `run` handler. The agent can now be called at `http://localhost:8080/WeatherAgent/run`.

The main difference compared to a standard Vercel AI agent is the use of the Restate Context at key points throughout the agent logic.
Any action with the Context is automatically recorded by the Restate Server and survives failures.
We use this for:
1. **Persisting LLM responses**: We wrap the model with the `durableCalls(ctx)` middleware, so that every LLM response is saved in Restate Server and can be replayed during recovery. The middleware is provided via the package [`@restatedev/vercel-ai-middleware`](https://github.com/restatedev/vercel-ai-middleware).
2. **Resilient tool execution**: Tools can make steps durable by using Context actions. Their outcome will then be persisted for recovery and retried until they succeed. `ctx.run` runs an action durably, retrying it until it succeeds (e.g. database interaction, API calls, non-deterministic actions).

<Tip> Unlike workflow engines, Restate’s durable function model lets you implement the agent as one long-running function. You don’t need to split LLM calls and tool calls into separate activities, avoiding the overhead of passing context and state around. </Tip>

<Accordion title={"Try it out"}>
    Ask for weather in Denver:
    ```bash
    curl localhost:8080/WeatherAgent/run \
    --json '{"prompt": "What is the weather like in Denver?"}'
    ```

    On the invocation page in the UI, you can see that your request is retrying because the weather API is down:

    <Frame>
        <img src="/img/tour/agents/weather-agent-stuck.png" alt="Invocation overview" />
    </Frame>

    To fix the problem, remove the line `failOnDenver` from the `fetchWeather` function in the `utils.ts` file:


    ```ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/utils.ts#weather"} 
export async function fetchWeather(city: string) {
  failOnDenver(city);
  const output = await fetchWeatherFromAPI(city);
  return parseWeatherResponse(output);
}
```

    Once you restart the service, the workflow finishes successfully.

</Accordion>

## Observability and Debugging

Restate includes a UI for monitoring and debugging your agents. The Invocations tab shows all agent executions with detailed traces of every LLM call, tool execution, and state change.

Go to the UI at `http://localhost:9070/ui/invocations` and click on your invocation id to see the complete timeline:
<Frame>
<img src="/img/tour/agents/weather-agent.png" alt="Invocation overview" />
</Frame>

<Accordion title={"OpenTelemetry Integration"}>
    Restate supports OpenTelemetry for exporting traces to external systems like Langfuse, DataDog, or Jaeger:

    Have a look at the [tracing docs](/operate/monitoring/tracing) to set this up.
</Accordion>

## Human-in-the-Loop Workflows

Many AI agents need human oversight for high-risk decisions or gathering additional input. Restate makes it easy to pause agent execution and wait for human input.

**Benefits with Restate:**
- If the agent crashes while waiting for human input, Restate continues waiting and recovers the promise on another process
- If the agent runs on function-as-a-service platforms, Restate suspends the function while its waiting, and resumes it later. You don't pay for the wait time.


Here's an insurance claim agent that asks for human approval for high-value claims:

```typescript humanintheloop/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/humanintheloop/agent.ts#here"} 
const { text } = await generateText({
  model,
  system:
    "You are an insurance claim evaluation agent." +
    "You are provided the amount and the reason, use these rules: " +
    "* if the amount is more than 1000, ask for human approval, " +
    "* if the amount is less than 1000, decide by yourself",
  prompt,
  tools: {
    humanApproval: tool({
      description: "Ask for human approval for high-value claims.",
      inputSchema: InsuranceClaimSchema,
      execute: async (claim: InsuranceClaim): Promise<boolean> => {
        const approval = ctx.awakeable<boolean>();
        await ctx.run("request-review", () =>
          requestHumanReview(
            `Please review: ${JSON.stringify(claim)}`,
            approval.id,
          ),
        );
        return approval.promise;
      },
    }),
  },
  stopWhen: [stepCountIs(5)],
  providerOptions: { openai: { parallelToolCalls: false } },
});
```

To implement human approval steps, you can use Restate's awakeables. An awakeable is a promise that can be resolved externally via an API call by providing its ID.
When you create the awakeable, you get back an ID and a promise. You can send the ID to the human approver, and then wait for the promise to be resolved.

You can also use awakeables outside of tools, for example, to implement human approval steps in between agent iterations.



<Accordion title={"Try it out"}>
    Start a request for a high-value claim that needs human approval.
    Use `/send` to start the claim asynchronously, without waiting for the result.
    ```bash
    curl localhost:8080/ClaimEvaluationAgent/run/send \
    --json '{"prompt": "Process my hospital bill of 3000USD for a broken leg."}'
    ```

    In the UI, you can see that the agent is waiting for human approval.

    You can restart the service to see how Restate continues waiting for the approval.

    Simulate approving the claim by executing the **curl request that was printed in the service logs**, similar to:

    ```bash
    curl localhost:8080/restate/awakeables/sign_1M28aqY6ZfuwBmRnmyP/resolve --json 'true'
    ```

    See in the UI how the workflow resumes and finishes after the approval.
    <Frame>
        <img src="/img/tour/agents/human-approval.png" alt="Invocation overview" />
    </Frame>
</Accordion>

<AccordionGroup>
<Accordion title={"Eliminate waiting costs on Function-as-a-Service platforms"}>
    The Restate Server takes care of persisting execution progress and promises.
    Your services themselves remain lightweight, stateless functions.

    With this setup, Restate can pause execution whenever needed and pick it up again later.

    This is particularly valuable on Function-as-a-Service platforms like AWS Lambda, where you pay for execution time.
    Instead of keeping a function running while it waits for a promise (approval, timers,...), Restate suspends it and then resumes once approval arrives.

    That way, you reduce costs and avoid hitting execution time limits.

    [Learn more](https://restate.dev/blog/we-replaced-400-lines-of-stepfunctions-asl-with-40-lines-of-typescript-by-making-lambdas-suspendable/)
</Accordion>

<Accordion title={"Timeouts and Escalation"}>
Add timeouts to human approval steps to prevent workflows from hanging indefinitely.

Restate persists the timer and the approval promise, so if the service crashes or is restarted, it will continue waiting with the correct remaining time:

```typescript humanintheloop/agent-with-timeout.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/humanintheloop/agent-with-timeout.ts#here"} 
try {
  // At most 3 hours, to reach our SLA
  const approved = await approval.promise.orTimeout({ hours: 3 });
  return { approved };
} catch (e) {
  if (e instanceof TimeoutError) {
    return {
      approved: false,
      reason: "Approval timed out - Evaluate with AI",
    };
  }
  throw e;
}
```

Try it out by sending a request to the service and killing the service process:
```bash
curl localhost:8080/ClaimEvaluationWithTimeoutsAgent/run/send \
--json '{"prompt": "Process my hospital bill of 3000USD for a broken leg."}'
```
You can see in the UI how the process will block for the remaining time without starting over.
</Accordion>
</AccordionGroup>

## Multi-turn Conversations and State

AI agents need to maintain conversation history and context across multiple interactions.

To implement stateful entities like chat sessions, or stateful agents, Restate provides Virtual Objects.

Each Virtual Object instance maintains isolated state and is identified by a unique key.

Here is an example of a Virtual Object that represents chat sessions:

```typescript multiturn/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/multiturn/agent.ts?collapse_prequel"} 
export default restate.object({
  name: "Chat",
  handlers: {
    message: async (ctx: restate.ObjectContext, req: { message: string }) => {
      const model = wrapLanguageModel({
        model: openai("gpt-4o"),
        middleware: durableCalls(ctx, { maxRetryAttempts: 3 }),
      });

      const messages =
        (await ctx.get<ModelMessage[]>("messages", superJson)) ?? [];
      messages.push({ role: "user", content: req.message });

      const res = await generateText({
        model,
        maxRetries: 0,
        system: "You are a helpful assistant.",
        messages,
      });

      ctx.set("messages", [...messages, ...res.response.messages], superJson);
      return { answer: res.text };
    },
    getHistory: shared(async (ctx: restate.ObjectSharedContext) =>
      ctx.get<ModelMessage[]>("messages", superJson)
    )
  },
});
```


Virtual Objects are ideal for implementing any entity with mutable state:
- **Durable state changes**: State changes are logged with Durable Execution, so they survive failures and are consistent with code execution
- **Built-in concurrency control**: Restate’s Virtual Objects have built-in queuing and consistency guarantees per object key. Handlers either have read-write access (`ObjectContext`) or read-only access (shared object context).
    - Only one handler with write access can run at a time per object key to prevent concurrent/lost writes or race conditions.
    - Handlers with read-only access can run concurrently to the write-access handlers.
- **Long-lived state**: K/V state is stored permanently. It has no automatic expiry. Clear it via `ctx.clear()`.
- **State is queryable** via the state tab in the UI.
<img src="/img/tour/agents/conversations.png" alt="Conversation State Management" />


<Accordion title={"Try it out"}>
    **Stateful Chat Agent:**

    Ask the agent to do some task:
    ```bash
    curl localhost:8080/Chat/session123/message \
    --json '{"message": "make a poem about durable execution"}'
    ```

    Continue the conversation - the agent remembers previous context:
    ```bash
    curl localhost:8080/Chat/session123/message \
    --json '{"message": "shorten it to 2 lines"}'
    ```

    Get conversation history or view it in the UI:
    ```bash
    curl localhost:8080/Chat/session123/getHistory
    ```

    **Seeing concurrency control in action:**

    In the chat service, the `message` handler is an exclusive handler, while the `getHistory` handler is a shared handler.

    Let's send some messages to a chat session:
    ```bash
    curl localhost:8080/Chat/session123/message/send --json '{"message": "make a poem about durable execution"}' &
    curl localhost:8080/Chat/session456/message/send --json '{"message": "what are the benefits of durable execution?"}' &
    curl localhost:8080/Chat/session789/message/send --json '{"message": "how does workflow orchestration work?"}' &
    curl localhost:8080/Chat/session123/message/send --json '{"message": "can you make it rhyme better?"}' &
    curl localhost:8080/Chat/session456/message/send --json '{"message": "what about fault tolerance in distributed systems?"}' &
    curl localhost:8080/Chat/session789/message/send --json '{"message": "give me a practical example"}' &
    curl localhost:8080/Chat/session101/message/send --json '{"message": "explain event sourcing in simple terms"}' &
    curl localhost:8080/Chat/session202/message/send --json '{"message": "what is the difference between async and sync processing?"}'
    ```

    The UI shows how Restate queues the requests per session to ensure consistency:
    <Frame>
        <img src="/img/tour/agents/exclusive-handlers.png" alt="Conversation State Management" />
    </Frame>

</Accordion>

<Accordion title={"Stateful Serverless Agents"}>
    You can run Virtual Objects on serverless platforms like AWS Lambda.
    When the request comes in, Restate attaches the correct state to the request, so your handler can access it locally.

    This way, you can implement stateful, serverless agents without managing any external state store and without worrying about concurrency issues.
</Accordion>

## Concurrent Tasks and Sub-workflows

Complex AI workflows often benefit from parallel execution, for example, running multiple agents simultaneously or breaking work into concurrent sub-tasks. This reduces overall execution time and allows specialized agents to work in parallel.

Here is an insurance claim agent that runs multiple analyses in parallel:

```typescript paralleltools/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/paralleltools/agent.ts?collapse_prequel"} 
export default restate.service({
  name: "ClaimApprovalAgent",
  handlers: {
    run: async (ctx: restate.Context, claim: InsuranceClaim) => {
      const model = wrapLanguageModel({
        model: openai("gpt-4o-mini"),
        middleware: durableCalls(ctx, { maxRetryAttempts: 3 }),
      });

      const response = await generateText({
        model,
        prompt: `Analyze the claim ${JSON.stringify(claim)}. 
        Decide whether to auto-approve or flag for human review.`,
        tools: {
          calculateMetrics: tool({
            description: "Calculate claim metrics.",
            inputSchema: InsuranceClaimSchema,
            execute: async (claim: InsuranceClaim) => {
              // Start analyses in parallel
              const eligibilityCheck = ctx.run("eligibility check", () =>
                doEligibilityCheck(claim),
              );
              const costReasonablenessScore = ctx.run(
                "cost reasonableness",
                () => compareToStandardRates(claim),
              );
              const fraudProbability = ctx.run("fraud check", () =>
                doFraudCheck(claim),
              );

              // Wait for all analyses to complete
              return RestatePromise.allSettled([
                eligibilityCheck,
                costReasonablenessScore,
                fraudProbability,
              ]);
            },
          }),
        },
        stopWhen: [stepCountIs(10)],
        providerOptions: { openai: { parallelToolCalls: false } },
      });
      return response.text;
    },
  },
});
```
Restate makes sure that all parallel tasks are retried and recovered until they succeed.

## Multi-Agent Orchestration


If each of the analysis tasks is complex, you can break them into separate sub-agents or workflows. Each sub-agent can be developed, deployed, and scaled independently.

Multi-agent systems coordinate specialized agents to solve complex problems. Restate provides reliable orchestration patterns where agents can communicate durably and maintain shared context.


### Sub-agents as tools

You can also dynamically offload specific tasks to sub-agents, by wrapping them as tools:

```typescript subagents/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/subagents/agent.ts?collapse_prequel"} 
export default restate.service({
  name: "ClaimAnalysisOrchestrator",
  handlers: {
    run: async (ctx: restate.Context, claim: InsuranceClaim) => {
      const model = wrapLanguageModel({
        model: openai("gpt-4o-mini"),
        middleware: durableCalls(ctx, { maxRetryAttempts: 3 }),
      });

      const decision = await generateText({
        model,
        prompt: `Analyze the claim ${claim} and use your tools to decide whether to approve.`,
        system: "You are a claim decision engine.",
        tools: {
          analyzeEligibility: tool({
            description: "Analyze eligibility result.",
            inputSchema: InsuranceClaimSchema,
            execute: async (claim: InsuranceClaim) =>
              ctx.serviceClient(eligibilityAgent).run(claim),
          }),
          analyzeCost: tool({
            description: "Compare cost to standard rates.",
            inputSchema: InsuranceClaimSchema,
            execute: async (claim: InsuranceClaim) =>
              ctx.serviceClient(rateComparisonAgent).run(claim),
          }),
          analyzeFraud: tool({
            description: "Analyze probability of fraud.",
            inputSchema: InsuranceClaimSchema,
            execute: async (claim: InsuranceClaim) =>
              ctx.serviceClient(fraudCheckAgent).run(claim),
          }),
        },
        stopWhen: [stepCountIs(10)],
        providerOptions: { openai: { parallelToolCalls: false } },
      });

      await ctx.run("notify", () => emailCustomer(decision.text));

      return decision.text;
    },
  },
});
```

### Parallel Sub-Agents
Here's a medical claim evaluation system with multiple specialized agents:


```typescript parallelagents/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/parallelagents/agent.ts?collapse_prequel"} 
export default restate.service({
  name: "ParallelClaimAnalyzer",
  handlers: {
    run: async (ctx: restate.Context, claim: InsuranceClaim) => {
      const [eligibility, rateComparison, fraudCheck] = await Promise.all([
        ctx.serviceClient(eligibilityAgent).run(claim),
        ctx.serviceClient(rateComparisonAgent).run(claim),
        ctx.serviceClient(fraudCheckAgent).run(claim),
      ]);

      const model = wrapLanguageModel({
        model: openai("gpt-4o-mini"),
        middleware: durableCalls(ctx, { maxRetryAttempts: 3 }),
      });

      const decision = await generateText({
        model,
        system: "You are a claim decision engine.",
        prompt: `Make final claim decision based on: 
                    Eligibility: ${eligibility}
                    Cost: ${rateComparison} 
                    Fraud: ${fraudCheck}`,
      });

      await ctx.run("notify", () => emailCustomer(decision.text));

      return decision.text;
    },
  },
});
```

You can also run these agents in parallel inside one of the tools.

## Error handling

Many agent SDKs include retry behavior settings. Restate also does retries via Durable Execution, so let's look at how these interact.

### Retries of LLM calls

LLM calls are expensive. To protect you from infinite retry loops and high LLM costs, you can configure the retry behavior of both Restate and the AI SDK you are using.

For the Vercel AI SDK, you can set `maxRetries` on `generateText` calls to retry LLM calls that fail due to rate limits or transient errors (defaults to 2).
After exhausting the retries, the agent will throw an error and fail.
Restate will then retry the invocation with exponential back-off to handle longer downtimes, faulty processes, or network communication issues.

To limit the number of retries done by Restate, you can set the `maxRetryAttempts` parameter on the `durableCalls` middleware:

```typescript errorhandling/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/errorhandling/agent.ts#max_attempts_example"} 
const model = wrapLanguageModel({
  model: openai("gpt-4o"),
  middleware: durableCalls(ctx, { maxRetryAttempts: 3 }),
});
```

For each retry issued by Restate, the Vercel AI SDK will retry the LLM call up to `maxRetries` times.
So with `maxRetryAttempts: 3` and `maxRetries: 2`, the LLM call will be attempted up to 6 times (3 Restate retries * 2 Vercel AI SDK attempts).


After Restate's retries are exhausted, the invocation will fail with a `TerminalError`, which is a Restate error type that indicates the failure is permanent and should not be retried anymore.
You can catch and handle terminal errors in your agent logic if needed (see below).


### Tool execution errors

The AI SDK wraps tool errors with an `ToolExecutionError`.
By default, Restate would infinitely retry this. this will unwrap any TerminalError's wrapped in a ToolExecutionError

```typescript errorhandling/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/errorhandling/agent.ts#option1"} 
const response1 = await generateText({
  model,
  tools: {
    getWeather: tool({
      description: "Get the current weather for a given city.",
      inputSchema: z.object({ city: z.string() }),
      execute: async ({ city }) => {
        return await ctx.run("get weather", () => fetchWeather(city));
      },
    }),
  },
  stopWhen: [stepCountIs(5)],
  system: "You are a helpful agent that provides weather updates.",
  messages: [{ role: "user", content: prompt }],
  providerOptions: { openai: { parallelToolCalls: false } },
});
```

```typescript errorhandling/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/errorhandling/agent.ts#option2"} 
const response2 = await generateText({
  model,
  tools: {
    getWeather: tool({
      description: "Get the current weather for a given city.",
      inputSchema: z.object({ city: z.string() }),
      execute: async ({ city }) => {
        return await ctx.run("get weather", () => fetchWeather(city));
      },
    }),
  },
  stopWhen: [stepCountIs(5)],
  onStepFinish: rethrowTerminalToolError,
  system: "You are a helpful agent that provides weather updates.",
  messages: [{ role: "user", content: prompt }],
});
```

```typescript errorhandling/agent.ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/new_tours/typescript/tutorials/tour-of-agents-typescript/src/errorhandling/agent.ts#option3"} 
const { steps, text } = await generateText({
  model,
  tools: {
    getWeather: tool({
      description: "Get the current weather for a given city.",
      inputSchema: z.object({ city: z.string() }),
      execute: async ({ city }) => {
        return await ctx.run("get weather", () => fetchWeather(city));
      },
    }),
  },
  stopWhen: [stepCountIs(5), hasTerminalToolError],
  system: "You are a helpful agent that provides weather updates.",
  messages: [{ role: "user", content: prompt }],
});

const terminalSteps = getTerminalToolSteps(steps);
if (terminalSteps.length > 0) {
  // Do something with the terminal tool error steps
}
```


## Advanced patterns

<AccordionGroup>
<Accordion title="Rolling back agent tasks on failure">
</Accordion>
<Accordion title="Long-running background agents">
</Accordion>
<Accordion title="Pub-sub to send steps back for user to see">
</Accordion>
<Accordion title="Streaming model responses live to UI">
</Accordion>
<Accordion title="Interrupting agents">
</Accordion>
</AccordionGroup>

## Summary

Restate transforms AI agent development by providing:

- **Durable Execution**: Automatic recovery from failures without losing progress
- **State Management**: Persistent conversation history and context
- **Simple Integration**: Works with OpenAI API and other LLM providers
- **Human-in-the-Loop**: Seamless approval workflows with timeouts  
- **Multi-Agent Coordination**: Reliable orchestration of specialized agents
- **Advanced Patterns**: Real-time progress updates, interruptions, and long-running workflows

Build resilient, observable AI agents that handle real-world complexity without the typical infrastructure overhead.
