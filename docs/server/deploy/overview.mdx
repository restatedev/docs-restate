---
title: "Deployment Overview"
description: "Choose the right deployment strategy for your Restate setup"
---

Restate is designed to be flexible in how you deploy it. This guide helps you choose the right deployment strategy based on your requirements.

## Deployment Decision Matrix

### Single-Node vs Cluster

| Requirement | Single-Node | Cluster |
|-------------|-------------|---------|
| **Durability** | ✅ | ✅ |
| **High Availability** | ❌ | ✅ |
| **Geo-replication** | ❌ | ✅ |
| **Horizontal Scaling** | ❌ | ✅ |
| **Vertical Scaling** | ✅ | ✅ |

#### When to Choose Single-Node
The current single node version provides you already with the highest level of durability (aggressively fsyncing changes to disk). So if you run it with a persistent volume, then you won't see any loss of data. In case of a crash, you will only see a short time of unavailability as the process gets restarted.

Single-node Restate runs as a single binary which persists its data to disk. Therefore, the durability of the disk defines how durable Restate is.

#### When to use Cluster Deployments
Distributed deployments with Restate serve multiple purposes:
- Higher availability, by having another process ready to take over, rather than waiting on the failed process (or container/pod) to be rescheduled. As a special case, this involves deployments across multiple availability zones or even regions that tolerate outages of full zones and regions.
- Better durability, by persisting copies of the data across multiple zones / regions.
- Scalability, by spreading load over multiple nodes.

### Storage Options

| Storage Type | Use Case | Durability | Performance |
|--------------|----------|------------|-------------|
| **Persistent Volumes** | Single-node, simple setup | Depends on underlying storage | High |
| **S3-Compatible Object Storage** | Multi-node clusters, backups | Very high | Good for snapshots |

#### When to Choose Persistent Volumes?
When you don't have access to an S3-compatible object store.

Single-node  Restate runs as a single binary which persists its data to disk. Therefore, the durability of the disk defines how durable Restate is.

#### Why Object Storage?
https://www.restate.dev/blog/building-a-modern-durable-execution-engine-from-first-principles
-->

Architectures that keep most- or all - of their data on object stores have become popular for many reasons: Object stores are unbeatable in terms of combined scalability, durability, and cost (AWS S3 cites eleven 9s of durability, stores more than 100 trillion objects, and is cheaper than persistent disks). Plus, the storage exists disaggregated from compute nodes, making the nodes stateless (or owning little state), which is highly desirable for efficient operations.

Object stores are also available in most on-prem setups we’ve encountered. It was natural for us to design Restate such that object storage would be the primary durability for the majority of the data.

Log, RocksDB, and object store
The reason why Restate has additionally a replication layer that persists new events (rather than writing events straight to object storage) is to provide low latency. Pure object store approaches have latencies that average around 100ms to make data durable, with tail latencies being a multiple of that. While that is feasible for analytical systems (e.g., Apache Flink) and data pipelines (like WarpStream), such latencies can quickly become prohibitive for many applications.

Restate’s replication bridges the latency gap between the requirements of fast durable functions and the capabilities of object storage.

- on AWS: use S3
- on Azure: use MinIO
- on GCP: use MinIO (GCS's S3-compatible API does not support the S3 conditional put semantics we require for both snapshots and metadata store usage. This requires native support, will have an update for you soon.)

## Choose Your Deployment Path

### I want simple, cost-effective deployment
**→ [Single-Node Deployment](/server/deploy/single-node)**
- Perfect for development, testing, or small production workloads
- Full durability with persistent volumes
- Vertical scaling capabilities

### I need high availability and scale
**→ [Cluster Deployment](/server/deploy/cluster)**
- Distributed across multiple nodes
- Fault tolerance and horizontal scaling
- Requires more complex configuration

### I'm using Kubernetes
**→ [Kubernetes Deployment](/server/deploy/kubernetes)**
- Helm charts for easy installation
- Restate Operator for advanced functionality
- Works with both single-node and cluster setups

## Platform-Specific Guides

### AWS
**→ [AWS Deployment](/server/deploy/aws)**
- EC2, ECS, EKS options
- Native S3 integration
- CDK templates available

### Azure
**→ [Azure Deployment](/server/deploy/azure)**
- Virtual Machines with managed disks
- MinIO for S3-compatible storage
- AKS integration

### Google Cloud
**→ [GCP Deployment](/server/deploy/gcp)**
- Compute Engine with persistent disks
- Cloud Storage via MinIO
- GKE support


## Load balancing
