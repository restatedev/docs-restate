---
title: "Deployment Overview"
sidebarTitle: "Overview"
description: "Choose the right deployment strategy for your Restate setup"
---

Restate is designed to be flexible in how you deploy it. We list the options on this page.

Restate runs as a single binary

[DIAGRAM]

## Single-Node vs Cluster

| Requirement | Single-Node | Cluster |
|-------------|-------------|---------|
| **Durability** | ✅ | ✅ |
| **High Availability** | ❌ | ✅ |
| **Geo-replication** | ❌ | ✅ |
| **Horizontal Scaling** | ❌ | ✅ |
| **Vertical Scaling** | ✅ | ✅ |

### When to Choose Single-Node
The current single node version provides you already with the highest level of durability (aggressively fsyncing changes to disk). So if you run it with a persistent volume, then you won't see any loss of data. In case of a crash, you will only see a short time of unavailability as the process gets restarted.

Single-node Restate runs as a single binary which persists its data to disk. Therefore, the durability of the disk defines how durable Restate is.

### When to use Cluster Deployments
Distributed deployments with Restate serve multiple purposes:
- Higher availability, by having another process ready to take over, rather than waiting on the failed process (or container/pod) to be rescheduled. As a special case, this involves deployments across multiple availability zones or even regions that tolerate outages of full zones and regions.
- Better durability, by persisting copies of the data across multiple zones / regions.
- Scalability, by spreading load over multiple nodes.

Restate stores data using two mechanisms: New events (invocations, journal entries, state updates, ...) are persisted in an embedded replicated log (called Bifrost). From there, events move to state indexes in RocksDB, which are periodically snapshotted to an object store. So at any point in time, the majority of the data is durable in the object store (the nodes maintain a copy as cache) while a smaller part of the data is durable in the log replicated across the nodes.

This is a form of storage tiering, though not the classical tiering like in modern logs. It is more similar to a database management system, where the write-ahead-log (WAL) would be replicated across nodes, while the table data files and indexes would be persisted on S3 (and cached on the nodes).


## Compute

The easiest way to manage the Restate cluster and services is via the Kubernetes operator.

We recommend using a managed Kubernetes provider such as EKS, AKS, or GKE for your Restate deployments.

Then, install the Restate operator on top of this, for easy management of Restate clusters and services.

This approach is also if you do a single node Restate deployment because it will manage the volume for you

## Storage Options

there is several stateful things in restate
- the metadata store.  by default its raft, but some folks use s3 for this, in which case s3 becomes a fairly critical component
- the partition stores, which are stored on disk, potentially replicated to speed up failover, but durability from that disk is not especially important and they snapshot to an object store, but that store is not on the hot path and is rarely read from
- the log stores, which are stored on disk and replicated, and must be durable

the snapshots can go to the filesystem (and if that filesystem is an nfs mount, then great)
the snapshot might only be done a few times an hour and read even more rarely. the performance of the snapshot destination has no effect on the performance of the cluster - nfs would be fine!
you can think of it more like backup than production database

| Storage Type | Use Case | Durability | Performance |
|--------------|----------|------------|-------------|
| **Persistent Volumes** | Single-node, simple setup | Depends on underlying storage | High |
| **S3-Compatible Object Storage** | Multi-node clusters, backups | Very high | Good for snapshots |

### Using Persistent Volumes
When you don't have access to an S3-compatible object store.

Single-node Restate runs as a single binary which persists its data to disk. Therefore, the durability of the disk defines how durable Restate is.



### Using Object Storage
https://www.restate.dev/blog/building-a-modern-durable-execution-engine-from-first-principles
-->

Architectures that keep most- or all - of their data on object stores have become popular for many reasons: Object stores are unbeatable in terms of combined scalability, durability, and cost (AWS S3 cites eleven 9s of durability, stores more than 100 trillion objects, and is cheaper than persistent disks). Plus, the storage exists disaggregated from compute nodes, making the nodes stateless (or owning little state), which is highly desirable for efficient operations.

Object stores are also available in most on-prem setups we’ve encountered. It was natural for us to design Restate such that object storage would be the primary durability for the majority of the data.

Log, RocksDB, and object store
The reason why Restate has additionally a replication layer that persists new events (rather than writing events straight to object storage) is to provide low latency. Pure object store approaches have latencies that average around 100ms to make data durable, with tail latencies being a multiple of that. While that is feasible for analytical systems (e.g., Apache Flink) and data pipelines (like WarpStream), such latencies can quickly become prohibitive for many applications.

Restate’s replication bridges the latency gap between the requirements of fast durable functions and the capabilities of object storage.

#### Supported Object Stores
Currently we support the following object stores:
- [AWS S3](/server/deploy/aws#using-s3)
- [MinIO](/server/deploy/kubernetes#minio-object-store) (only for snapshots, not metadata)
- Coming soon: Google Cloud Storage and Azure Blob Storage (Contact us for more info: [Discord](https://discord.com/invite/skW3AZ6uGd)/[Slack](https://join.slack.com/t/restatecommunity/shared_invite/zt-2v9gl005c-WBpr167o5XJZI1l7HWKImA))

## Platform-Specific Guides

### Kubernetes
**→ [Kubernetes Deployment](/server/deploy/kubernetes)**
- Helm charts for easy installation
- Restate Operator for advanced functionality
- Works with both single-node and cluster setups

### AWS
**→ [AWS Deployment](/server/deploy/aws)**
- Recommended: EKS on Fargate with Restate Operator
- Snapshot storage on S3
- CDK templates available

### Azure
**→ [Azure Deployment](/server/deploy/azure)**
- Recommended: AKS with Restate Operator
- MinIO for S3-compatible snapshot storage (until native Azure Blob support is available)

### Google Cloud
**→ [GCP Deployment](/server/deploy/gcp)**
- Recommended: GKE with Restate Operator
- MinIO for S3-compatible snapshot storage (until native GCS support is available)

## Load balancing

If you run a multi-node Restate cluster, we recommend using a cloud-native load balancer to distribute incoming requests across the nodes.


## Creating backups

visit the [snapshots/backups docs](/server/operations/snapshots) for details on how to create backups of your data.