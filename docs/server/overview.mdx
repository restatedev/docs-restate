---
title: "Deployment Overview"
sidebarTitle: "Overview"
description: "Choose the right deployment strategy for your Restate setup"
---


Restate can run as a single binary, making it easy to run it on a variety of platforms, from your local machine, to a VM, to Kubernetes.

This page gives an overview of the different deployment strategies and requirements.

## Infrastructure and Storage

Restate has a layered architecture:

<Frame>
    <img
        alt={"Restate Node internal architecture"}
        src={"/img/architecture/restate_layered_arch.png"}
        className="bg-white p-4 rounded-lg"
    />
</Frame>


The architecture has the following stateful components:

- [The metadata store](/references/architecture#metadata-store): Restate's control plane includes a built-in Raft-based metadata store. This store keeps track of the cluster configuration, partition assignments, and other critical metadata. It is essential to the correctness of the overall system.
- [The durable log](/references/architecture#durability-and-storage-model): The log is the primary durability layer. New events (invocations, journal entries, state updates, ...) are first persisted in the log, and then move to the partition processors. The log is stored on disk and replicated. Durability is critical here.
- [The partition processor stores](/references/architecture#materializing-state-for-fast-access): The partition processor leader maintains a full cache of the partition’s materialized state in RocksDB for fast reads and updates during execution. This cache can always be rebuilt from the log, so durability is not critical here. In cluster deployments, the content of RocksDB should be periodically snapshotted to an object store, for faster failover.

### Persistent Volumes

Single-node Restate runs as a single binary which persists its data to disk. Therefore, the durability of the disk defines how durable Restate is.

When running a single-node Restate Server, you might want to take periodic backups of the persistent volume (see [data backups](/server/snapshots#data-backups-2)).

### Object Storage for snapshots

As we mentioned, the content of the partition processor's RocksDB should be periodically snapshotted for faster failover.

**Snapshotting is mandatory for Restate clusters**, because it enables handing over partitions between nodes.

Restate supports snapshotting to object stores because they offer great scalability, durability, and cost and are available in all cloud providers and most on-prem setups.
Plus, the storage exists disaggregated from compute nodes, making the nodes stateless (or owning little state), which is highly desirable for efficient operations.

Performance requirements for snapshot storage are minimal. Snapshots typically occur only a few times per hour and are read rarely, so the snapshot destination's performance doesn't affect cluster performance.

See the [snapshot documentation](/server/snapshots#snapshots-2) for more information.

<Note>
    It is possible to also keep the metadata store in object storage, but this has higher requirements than just S3-compatibility, because this critical component requires strong consistency.
    This is currently only offered by AWS S3.
</Note>

#### Supported Object Stores
Currently we support the following object stores:
- [AWS S3](/server/deploy/aws#using-s3)
- [MinIO](/server/deploy/kubernetes#minio-object-store) (only for snapshots, not metadata)
- Coming soon: Google Cloud Storage and Azure Blob Storage (Contact us for more info: [Discord](https://discord.com/invite/skW3AZ6uGd)/[Slack](https://join.slack.com/t/restatecommunity/shared_invite/zt-2v9gl005c-WBpr167o5XJZI1l7HWKImA))

## Platform-Specific Deployment Guides

The easiest way to manage the Restate cluster and services is via the Kubernetes operator, since it gives you a lot of built-in functionality and tooling.
Therefore, **we recommend using the Restate Kubernetes Operator in combination with a managed Kubernetes provider such as AWS EKS, Azure's AKS, or Google's GKE.**

This approach is also recommended for single-node Restate deployments because it will make managing the persistent volume of your Restate instance easier.
If your instance goes down, the Kubernetes provider automatically restarts it and reattaches the persistent volume.

Follow the links below for platform-specific deployment instructions:

- [Kubernetes Deployment](/server/deploy/kubernetes)
- [AWS Deployment](/server/deploy/aws)
- [Azure Deployment](/server/deploy/azure)
- [GCP Deployment](/server/deploy/gcp)

## Single-Node vs. Cluster Deployments
Restate can be deployed as a single-node setup or as a distributed cluster, depending on your requirements for availability and scalability.

| Requirement | Single-Node | Cluster |
|-------------|-------------|---------|
| **Durability** | ✅ | ✅ |
| **High Availability** | ❌ | ✅ |
| **Geo-replication** | ❌ | ✅ |
| **Horizontal Scaling** | ❌ | ✅ |
| **Vertical Scaling** | ✅ | ✅ |

### When to Choose Single-Node
Single-node Restate runs as a single binary that persists data to disk. To ensure durability, it is essential to use a persistent volume for storing all Restate data.

Despite being a single-node setup, this setup does not sacrifice durability. Restate aggressively fsyncs data to disk, providing the **highest level of durability**.
The durability of the disk defines how durable Restate is.

In case of a crash, you won't lose data, but you will experience a short window of unavailability while the process restarts.

**Use this for**: development, testing, or production workloads which can tolerate short downtimes and do not require high throughput (horizontal scaling).

### When to use Cluster Deployments
Restate cluster deployments serve multiple purposes:
- **High-availability**: In case of a crash, other nodes of the cluster can take over immediately, rather than waiting on the failed node to get rescheduled.
- **Geo-replication**: To reduce the risk of downtime even further, Restate clusters can run in a geo-replicated setup, with nodes deployed across multiple availability zones or even regions. Letting you tolerate outages of full zones and regions.
- **Scalability** by spreading load over multiple nodes.
- Even **better durability**, by persisting copies of the data across multiple zones / regions.

**Use this for**: use cases requiring high availability, very high throughput, or geo-replication.