---
title: "AI Agent Quickstart"
sidebarTitle: "Restate + Agent SDK"
description: "Build and run your first AI agent with Restate and popular AI SDKs"
icon: "robot"
---

import {GitHubLink} from "/snippets/blocks/github-link.mdx";
import InstallRestate from "/snippets/common/install-server-without-running-cli.mdx";

This guide takes you through building your first AI agent with Restate and popular AI SDKs.

We will run a simple weather agent that can answer questions about the weather using durable execution to ensure reliability.

<img src="/img/quickstart/agent-quickstart/weather-agent.png" alt="AI Agent Quickstart" noZoom/>

Select your AI SDK:

<Tabs>
<Tab title={"TypeScript + Vercel AI"} icon={"/img/languages/typescript.svg"}>
<Info>
**Prerequisites**:
- [Node.js](https://nodejs.org/en/) >= v20
- OpenAI API key (get one at [OpenAI](https://platform.openai.com/))
</Info>
<Steps>
<Step title="Install Restate Server & CLI">
<InstallRestate/>
</Step>

<Step title="Get the AI Agent template">
```shell
git clone https://github.com/restatedev/ai-examples.git &&
cd ai-examples/vercel-ai/template &&
npm install
```
<GitHubLink url={"https://github.com/restatedev/ai-examples/tree/main/vercel-ai/template"}/>
This template implements a weather agent using the [Vercel AI SDK](https://ai-sdk.dev/docs/foundations/overview) and Restate.
</Step>

<Step title="Set your OpenAI API key">
```shell
export OPENAI_API_KEY=your_openai_api_key_here
```
</Step>

<Step title="Run the AI Agent service">
```shell
npm run dev
```

The weather agent is now listening on port 9080.
</Step>

<Step title="Register the service">
Tell Restate where the service is running (`http://localhost:9080`), so Restate can discover and register the services and handlers behind this endpoint.
You can do this via the UI (`http://localhost:9070`) or via:

<CodeGroup>
```shell CLI
restate deployments register http://localhost:9080
```
```shell curl
curl localhost:9070/deployments --json '{"uri": "http://localhost:9080"}'
```
</CodeGroup>

<Expandable title={"Output"}>
<CodeGroup>
```shell CLI
❯ SERVICES THAT WILL BE ADDED:
- agent
Type: Service
HANDLER  INPUT                                     OUTPUT
run      value of content-type 'application/json'  value of content-type 'application/json'


✔ Are you sure you want to apply those changes? · yes
✅ DEPLOYMENT:
SERVICE  REV
agent    1
```

```shell curl
{
    "id": "dp_17sztQp4gnEC1L0OCFM9aEh",
    "services": [
        {
            "name": "Agent",
            "handlers": [
                {
                    "name": "run",
                    "ty": "Shared",
                    "input_description": "one of [\"none\", \"value of content-type 'application/json'\"]",
                    "output_description": "value of content-type 'application/json'"
                }
            ],
            "ty": "Service",
            "deployment_id": "dp_17sztQp4gnEC1L0OCFM9aEh",
            "revision": 1,
            "public": true,
            "idempotency_retention": "1day"
        }
    ]
}
```
</CodeGroup>
</Expandable>

If you run Restate with Docker, register `http://host.docker.internal:9080` instead of `http://localhost:9080`.

<Accordion title="Restate Cloud">
When using [Restate Cloud](https://restate.dev/cloud), your service must be accessible over the public internet so Restate can invoke it.
If you want to develop with a local service, you can expose it using our [tunnel](/deploy/server/cloud/#registering-restate-services-with-your-environment) feature.
</Accordion>
</Step>

<Step title="Send weather requests to the AI Agent">
Invoke the agent via the Restate UI playground: go to `http://localhost:9070`, click on your service and then on playground.
<Frame>
<img src={"/img/quickstart/agent-quickstart/vercel-playground.png"} alt="Restate UI Playground"/>
</Frame>

Or invoke via `curl`:
```shell
curl localhost:8080/agent/run --json '"What is the weather in Detroit?"'
```
Output: `The weather in Detroit is currently 17°C with misty conditions.`.
</Step>

<Step title="Congratulations, you just ran a Durable AI Agent!">

The agent you just invoked uses Durable Execution to make AI tool calls resilient to failures.
Each tool call (like fetching weather data) is durably executed and can be retried if needed.

Have a look at the Invocations tab in the Restate UI to see the journal entries created for each step:
<Frame>
<img src={"/img/quickstart/agent-quickstart/vercel-trace.png"} alt="Restate UI Journal Entries"/>
</Frame>

```ts expandable {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/main/vercel-ai/template/src/app.ts?collapse_imports"} 
async function simpleAgent(restate: restate.Context, prompt: string) {
  // we wrap the model with the 'durableCalls' middleware, which
  // stores each response in Restate's journal, to be restored on retries
  const model = wrapLanguageModel({
    model: openai("gpt-4o-2024-08-06"),
    middleware: durableCalls(restate, { maxRetryAttempts: 3 }),
  });

  const { text } = await generateText({
    model,
    prompt,
    tools: {
      getWeather: tool({
        description: "Get the current weather for a given city.",
        inputSchema: z.object({ city: z.string() }),
        execute: async ({ city }) => {
          // call tool wrapped as Restate durable step
          return await restate.run("get weather", () => fetchWeather(city));
        },
      }),
    },
    stopWhen: [stepCountIs(5)],
    // these are local retries by the AI SDK
    // Restate will retry the invocation once those local retries are exhaused to
    // handle longer downtimes, faulty processes, or network communication issues
    maxRetries: 3,
    system: "You are a helpful agent.",
    providerOptions: { openai: { parallelToolCalls: false } },
  });

  return text;
}

// create a simple Restate service as the callable entrypoint
// for our durable agent function
const agent = restate.service({
  name: "agent",
  handlers: {
    run: async (ctx: restate.Context, prompt: string) => {
      return simpleAgent(ctx, prompt);
    },
  },
});

// we serve the entry-point via an HTTP/2 server
restate.serve({
  services: [agent],
});
```

<GitHubLink url={"https://github.com/restatedev/ai-examples/blob/main/vercel-ai/template/src/app.ts"}/>


<Accordion title="See how a failing tool call is retried">
Ask about the weather in Denver:
```shell
curl localhost:8080/Agent/run --json '"What is the weather in Denver?"'
```

You can see in the service logs and in the Restate UI how each LLM call and tool step gets durably executed.
We can see how the weather tool is currently stuck, because the weather API is down.

<Frame>
<img src={"/img/quickstart/agent-quickstart/vercel-agent-stuck.png"} alt="Restate UI Durable Execution"/>
</Frame>

This was a mimicked failure. To fix the problem, remove the line `failOnDenver` from the `fetchWeather` function in the `utils.ts` file:

```ts {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/main/vercel-ai/template/src/utils/weather.ts#weather"}
export async function fetchWeather(city: string) {
failOnDenver(city);
const output = await fetchWeatherFromAPI(city);
return parseWeatherResponse(output);
}
```

Once you restart the service, the agent resumes at the weather tool call and successfully completes the request.

</Accordion>

    **Next step:**
Follow the [Tour of Agents](/tour/ai-agents) to learn how to build agents with Restate and Vercel AI SDK.

</Step>
</Steps>
</Tab>
<Tab title={"Python + OpenAI"} icon={"/img/languages/python.svg"}>
<Info>
**Prerequisites**:
- Python >= v3.12
- [uv](https://docs.astral.sh/uv/getting-started/installation/)
- OpenAI API key (get one at [OpenAI](https://platform.openai.com/))
</Info>
<Steps>
<Step title="Install Restate Server & CLI">
<InstallRestate/>
</Step>

<Step title="Get the AI Agent template">
```shell
git clone https://github.com/restatedev/ai-examples.git &&
cd ai-examples/openai-agents/template
```
<GitHubLink url={"https://github.com/restatedev/ai-examples/tree/main/openai-agents/template"}/>
</Step>

<Step title="Set your OpenAI API key">
```shell
export OPENAI_API_KEY=your_openai_api_key_here
```
</Step>

<Step title="Run the AI Agent service">
```shell
uv run .
```

The weather agent is now listening on port 9080.
</Step>

<Step title="Register the agent service">
Tell Restate where the service is running (`http://localhost:9080`), so Restate can discover and register the services and handlers behind this endpoint.
You can do this via the UI (`http://localhost:9070`) or via:

<CodeGroup>
```shell CLI
restate deployments register http://localhost:9080
```
```shell curl
curl localhost:9070/deployments --json '{"uri": "http://localhost:9080"}'
```
</CodeGroup>

<Expandable title={"Output"}>
<CodeGroup>
```shell CLI
❯ SERVICES THAT WILL BE ADDED:
- Agent
Type: Service
HANDLER  INPUT                                     OUTPUT
run      value of content-type 'application/json'  value of content-type 'application/json'


✔ Are you sure you want to apply those changes? · yes
✅ DEPLOYMENT:
SERVICE  REV
Agent    1
```

```shell curl
{
    "id": "dp_17sztQp4gnEC1L0OCFM9aEh",
    "services": [
        {
            "name": "Agent",
            "handlers": [
                {
                    "name": "run",
                    "ty": "Shared",
                    "input_description": "one of [\"none\", \"value of content-type 'application/json'\"]",
                    "output_description": "value of content-type 'application/json'"
                }
            ],
            "ty": "Service",
            "deployment_id": "dp_17sztQp4gnEC1L0OCFM9aEh",
            "revision": 1,
            "public": true,
            "idempotency_retention": "1day"
        }
    ]
}
```
</CodeGroup>
</Expandable>

If you run Restate with Docker, register `http://host.docker.internal:9080` instead of `http://localhost:9080`.

<Accordion title="Restate Cloud">
When using [Restate Cloud](https://restate.dev/cloud), your service must be accessible over the public internet so Restate can invoke it.
If you want to develop with a local service, you can expose it using our [tunnel](/deploy/server/cloud/#registering-restate-services-with-your-environment) feature.
</Accordion>
</Step>

<Step title="Send weather requests to the AI Agent">
Invoke the agent via the Restate UI playground: go to `http://localhost:9070`, click on your service and then on playground.
<Frame>
<img src={"/img/quickstart/playground.png"} alt="Restate UI Playground"/>
</Frame>

Or invoke via `curl`:
```shell
curl localhost:8080/Agent/run --json '"What is the weather in Detroit?"'
```
Output: `The weather in Detroit is currently 17°C with misty conditions.`

</Step>

<Step title="Congratulations, you just ran a Durable AI Agent!">

The agent you just invoked uses Durable Execution to make AI tool calls resilient to failures.
Each tool call (like fetching weather data) is durably executed and can be retried if needed.


```python expandable {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/main/openai-agents/template/agent.py?collapse_imports"} 
class WeatherRequest(BaseModel):
    """Request to get the weather for a city."""
    city: str


@function_tool
async def get_weather(
    wrapper: RunContextWrapper[restate.Context], req: WeatherRequest
) -> WeatherResponse:
    """Get the current weather for a given city."""
    # Do durable steps using the Restate context
    restate_context = wrapper.context
    resp = await restate_context.run(
        "Get weather", fetch_weather, args=(req.city,))
    return await parse_weather_data(resp)


my_agent = Agent[restate.Context](
    name="Helpful Agent",
    handoff_description="A helpful agent.",
    instructions="You are a helpful agent.",
    tools=[get_weather],
)


# Agent keyed by conversation id
agent = restate.Service("Agent")


@agent.handler()
async def run(restate_context: restate.Context, message: str) -> str:

    result = await Runner.run(
        my_agent,
        input=message,
        # Pass the Restate context to tools to make tool execution steps durable
        context=restate_context,
        # Choose any model and let Restate persist your calls
        run_config=RunConfig(
            model="gpt-4o",
            model_provider=DurableModelCalls(restate_context)
        ),
    )

    return result.final_output
```
<GitHubLink url={"https://github.com/restatedev/ai-examples/blob/main/openai-agents/template/agent.py"}/>

<Accordion title="See how a failing tool call is retried">
Ask about the weather in Denver:
```shell
curl localhost:8080/agent/run --json '"What is the weather in Denver?"'
```

You can see in the service logs and in the Restate UI how each LLM call and tool step gets durably executed.
We can see how the weather tool is currently stuck, because the weather API is down.

<Frame>
<img src={"/img/quickstart/agent-quickstart/openai-agent-stuck.png"} alt="Restate UI Durable Execution"/>
</Frame>

This was a mimicked failure. To fix the problem, remove the line `fail_on_denver` from the `fetch_weather` function in the `utils.py` file:

```python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/main/openai-agents/template/utils/utils.py#weather"}
export async function fetchWeather(city: string) {
failOnDenver(city);
const output = await fetchWeatherFromAPI(city);
return parseWeatherResponse(output);
}
```


Once you restart the service, the agent resumes at the weather tool call and successfully completes the request.

</Accordion>
</Step>
</Steps>
</Tab>
</Tabs>