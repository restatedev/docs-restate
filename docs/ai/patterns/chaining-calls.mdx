---
title: "Chaining LLM calls"
description: "Build fault-tolerant processing pipelines with automatic retries and recovery."
tags: ["recipe"]
---

import StartExample from "/snippets/ai-guides/start-example.mdx";

Build fault-tolerant processing pipelines where each step transforms the previous step's output.
If any step fails, Restate automatically resumes from that exact point.

For example:
```
+----------+    +---------------+    +-------------+    +--------------+
|  Input   | => | Extract       | => | Sort        | => | Format as    |
| Message  |    | Metrics       |    | Results     |    | Table        |
+----------+    +---------------+    +-------------+    +--------------+
```

## How does Restate help?
The benefits of using Restate here are:
- **Automatic retries** of failed tasks: LLM API down, timeouts, infrastructure failures, etc.
- **Recovery of previous progress**: After a failure, Restate recovers the progress the execution did before the crash.
- Works with **any AI SDK** (OpenAI, LangChain, Pydantic AI, LiteLLM, etc.) and **any programming language** supported by Restate (TypeScript, Python, Go, etc.).

## Example

Wrap each step in the chain with `ctx.run()` to ensure fault tolerance and automatic recovery. If any step fails, Restate will retry from that exact point without losing previous progress.

<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/examples/refs/heads/main/typescript/patterns-use-cases/src/parallelizework/fan_out_worker.ts?collapse_prequel"}
async function processReport(
  ctx: Context,
  { message }: { message: string },
) {
  // Step 1: Process the initial input with the first prompt
  const result = await ctx.run("Extract metrics", async () =>
    llmCall(`Extract only the numerical values and their associated metrics from the text.
            Format each as 'metric name: metric' on a new line. Input: ${message}`),
    {maxRetryAttempts: 3}
  );

  // Step 2: Process the result from Step 1
  const result2 = await ctx.run("Sort metrics", async () =>
    llmCall(
      `Sort all lines in descending order by numerical value. Input: ${result}`,
    ),
    {maxRetryAttempts: 3}
  );

  // Step 3: Process the result from Step 2
  return ctx.run("Format as table", async () =>
    llmCall(
      `Format the sorted data as a markdown table with columns 'Metric Name' and 'Value'. Input: ${result2}`,
    ),
    {maxRetryAttempts: 3}
  );
}
```

```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/main/python-patterns/app/chaining.py?collapse_prequel"}
@call_chaining_svc.handler()
async def run(ctx: restate.Context, prompt: Prompt) -> str | None:
    """Chains multiple LLM calls sequentially, where each step processes the previous step's output."""

    # Step 1: Process the initial input with the first prompt
    result = await ctx.run_typed(
        "Extract metrics",
        llm_call,
        RunOptions(max_attempts=3),
        prompt=f"Extract only the numerical values and their associated metrics from the text. "
        f"Format each as 'metric name: metric' on a new line. Input: {prompt.message}",
    )

    # Step 2: Process the result from Step 1
    result2 = await ctx.run_typed(
        "Sort metrics",
        llm_call,
        RunOptions(max_attempts=3),
        prompt=f"Sort all lines in descending order by numerical value. Input: {result}",
    )

    # Step 3: Process the result from Step 2
    result3 = await ctx.run_typed(
        "Format as table",
        llm_call,
        RunOptions(max_attempts=3),
        prompt=f"Format the sorted data as a markdown table with columns 'Metric Name' and 'Value'. Input: {result2}",
    )

    return result3.content
```

</CodeGroup>

View on GitHub: [TS](https://github.com/restatedev/ai-examples/blob/typescript_patterns/typescript-patterns/src/chaining.ts) /
[Python](https://github.com/restatedev/ai-examples/blob/main/python-patterns/app/chaining.py)


<Tip>
    This pattern is implementable with any of our SDKs and any AI SDK.
    If you need help with a specific SDK, please reach out to us via [Discord](https://discord.com/invite/skW3AZ6uGd) or [Slack](https://join.slack.com/t/restatecommunity/shared_invite/zt-2v9gl005c-WBpr167o5XJZI1l7HWKImA).
</Tip>


## Running the example

<Steps>
<StartExample/>
<Step title="Send a request">

    <Tabs>
        <Tab title="UI">
            In the UI (`http://localhost:9070`), click on the `run` handler of the `CallChainingService` to open the playground and send a default request:

            <img src="https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/main/doc/img/patterns/chaining_playground.png" alt="Chaining LLM calls - UI"/>

        </Tab>
        <Tab title="curl">
            Send a request to the service:

            <CodeGroup>
                ```shell TypeScript
                curl -X POST http://localhost:8080/CallChainingService/run \
                --json '{"message": "Our sales increased by 25% this quarter. We had 1,200 new customers and revenue of $450,000. Customer satisfaction dropped to 78% from last quarters 85%."}'
                ```
                ```shell Python
                curl -X POST http://localhost:8080/CallChainingService/run \
                --json '{"message": "Our sales increased by 25% this quarter. We had 1,200 new customers and revenue of $450,000. Customer satisfaction dropped to 78% from last quarters 85%."}'
                ```
            </CodeGroup>
        </Tab>
    </Tabs>



</Step>

<Step title="Check the Restate UI">

    You see in the Invocations Tab of the UI how the LLM is called multiple times, and how each result is persisted in Restate:
    <img src="https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/main/doc/img/patterns/chaining.png" alt="Chaining LLM calls - UI"/>

</Step>
</Steps>

