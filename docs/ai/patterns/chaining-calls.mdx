---
title: "Chaining LLM calls"
description: "Build fault-tolerant processing pipelines with automatic retries and recovery."
tags: ["recipe"]
---

import StartExample from "/snippets/ai-guides/start-example.mdx";

Build fault-tolerant processing pipelines where each step transforms the previous step's output.
If any step fails, Restate automatically resumes from that exact point.

For example:
```
+----------+    +---------------+    +-------------+    +--------------+
|  Input   | => | Extract       | => | Sort        | => | Format as    |
| Message  |    | Metrics       |    | Results     |    | Table        |
+----------+    +---------------+    +-------------+    +--------------+
```

## How does Restate help?
The benefits of using Restate here are:
- **Automatic retries** of failed tasks: LLM API down, timeouts, infrastructure failures, etc.
- **Recovery of previous progress**: After a failure, Restate recovers the progress the execution did before the crash.
- Works with **any AI SDK** (OpenAI, LangChain, Pydantic AI, LiteLLM, etc.) and **any programming language** supported by Restate (TypeScript, Python, Go, etc.).

## Example

Wrap each step in the chain with `ctx.run()` to ensure fault tolerance and automatic recovery. Restate uses durable execution to persist the result of each step as it completes, so if any step fails, Restate will retry from that exact point without losing previous progress or re-executing completed steps.

<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/typescript-patterns/src/chaining.ts#here"} 
async function processReport(ctx: Context, report: { message: string }) {
  // Step 1: Extract metrics
  const extract = await ctx.run(
    "Extract metrics",
    async () =>
      llmCall(`Extract numerical values and their metrics from the text. 
            Format as 'Metric: Value' per line. Input: ${report.message}`),
    { maxRetryAttempts: 3 },
  );

  // Step 2: Process the result from Step 1
  const sortedMetrics = await ctx.run(
    "Sort metrics",
    async () =>
      llmCall(`Sort lines in descending order by value. Input: ${extract}`),
    { maxRetryAttempts: 3 },
  );

  // Step 3: Format as table
  return ctx.run(
    "Format as table",
    async () =>
      llmCall(
        `Format the data as a markdown table with columns 
         'Metric Name' and 'Value'. Input: ${sortedMetrics}`,
      ),
    { maxRetryAttempts: 3 },
  );
}
```

```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/python-patterns/app/chaining.py?collapse_prequel"} 
call_chaining_svc = restate.Service("CallChainingService")


@call_chaining_svc.handler()
async def process_report(ctx: restate.Context, report: Report) -> str | None:
    """Sequentially chains multiple LLM calls, each transforming the prior output."""

    # Step 1: Extract metrics
    extract = await ctx.run_typed(
        "Extract metrics",
        llm_call,
        RunOptions(max_attempts=3),
        prompt=(
            "Extract numerical values and their metrics from the text. "
            f"Format as 'Metric: Value' per line. Input: {report.text}"
        ),
    )

    # Step 2: Sort by value
    sorted_metrics = await ctx.run_typed(
        "Sort metrics",
        llm_call,
        RunOptions(max_attempts=3),
        prompt=f"Sort lines in descending order by value. Input: {extract}",
    )

    # Step 3: Format as table
    table = await ctx.run_typed(
        "Format as table",
        llm_call,
        RunOptions(max_attempts=3),
        prompt=(
            "Format the data as a markdown table with columns "
            f"'Metric Name' and 'Value'. Input: {sorted_metrics}"
        ),
    )

    return table.content
```

</CodeGroup>

View on GitHub: [TS](https://github.com/restatedev/ai-examples/blob/typescript_patterns/typescript-patterns/src/chaining.ts) /
[Python](https://github.com/restatedev/ai-examples/blob/main/python-patterns/app/chaining.py)


<Tip>
    This pattern is implementable with any of our SDKs and any AI SDK.
    If you need help with a specific SDK, please reach out to us via [Discord](https://discord.com/invite/skW3AZ6uGd) or [Slack](https://join.slack.com/t/restatecommunity/shared_invite/zt-2v9gl005c-WBpr167o5XJZI1l7HWKImA).
</Tip>


## Running the example

<Steps>
<StartExample/>
<Step title="Send a request">

    <Tabs>
        <Tab title="UI">
            In the UI (`http://localhost:9070`), click on the `run` handler of the `CallChainingService` to open the playground and send a default request:

            <img src="/img/ai/patterns/chaining_playground.png" alt="Chaining LLM calls - UI"/>

        </Tab>
        <Tab title="curl">
            Send a request to the service:

            <CodeGroup>
                ```shell TypeScript
                curl localhost:8080/CallChainingService/process_report \
                --json '{"text": "Our sales increased by 25% this quarter. We had 1,200 new customers and revenue of $450,000. Customer satisfaction dropped to 78% from last quarters 85%."}'
                ```
                ```shell Python
                curl localhost:8080/CallChainingService/process_report \
                --json '{"text": "Our sales increased by 25% this quarter. We had 1,200 new customers and revenue of $450,000. Customer satisfaction dropped to 78% from last quarters 85%."}'
                ```
            </CodeGroup>
        </Tab>
    </Tabs>
</Step>

<Step title="Check the Restate UI">

    You see in the Invocations Tab of the UI how the LLM is called multiple times, and how each result is persisted in Restate:
    <img src="/img/ai/patterns/chaining.png" alt="Chaining LLM calls - UI"/>

</Step>
</Steps>

