---
title: "Multi-Agent Systems"
sidebarTitle: "Multi-Agent Systems"
description: "Implement recoverable routing of tasks to tools and agents with Restate."
tags: ["recipe"]
---

import StartExample from "/snippets/ai-guides/start-example.mdx";

Dynamically route requests to appropriate tools and agents based on context and intent. Restate ensures that all routing decisions, tool calls, and agent interactions are durably logged, enabling automatic retries and recovery from failures.

## How does Restate help?
At the core of routing is an agent loop that repeatedly calls the LLM, checks for tool/agent calls, executes them, and feeds the results back to the LLM until a final answer is produced.

### Durable routing decisions
Restate lets you write standard code while making it resilient through durable execution. The key is wrapping LLM calls and tool execution steps with Restate Context actions.

This design means you don’t need any special logic to implement routing: if the LLM decides to forward a request to a tool or agent, the decision is automatically persisted in Restate’s log, and the workflow can be retried or resumed after failures.

### Durable calls to remote agents and tools

For distributed systems, Restate also provides **durable messaging and RPC**, enabling multi-agent systems with resilient communication.
Restate extends Durable Execution with messaging and RPC, making remote agent calls look like normal function calls, while providing end-to-end resilience.

- Agents and tools can run as independent services across different infrastructures (Kubernetes, Vercel, AWS Lambda, etc.), in different languages
- Communication is durable and failure-resistant
- Work is idempotent, avoiding duplicate processing
- The calling agent can suspend while awaiting results
- Reliably schedule delayed or periodic tasks


## Routing to local agents

Route requests to specialized agents based on LLM outputs.

Routing to a local agent is similar to routing to local tools, but instead of calling a tool function, you call the LLM again with a specialized prompt for the selected agent.

<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/typescript-patterns/src/routing-to-agent.ts#here"} 
const tools = {
  billingAgent: tool({
    description: "Expert in payments, charges, and refunds",
    inputSchema: z.object({}),
  }),
  accountAgent: tool({
    description: "Expert in login issues and security",
    inputSchema: z.object({}),
  }),
  productAgent: tool({
    description: "Expert in features and how-to guides",
    inputSchema: z.object({}),
  }),
} as const;
type Specialist = keyof typeof tools;

const PROMPTS = {
  billingAgent:
    "You are a billing support agent specializing in payments, charges, and refunds.",
  accountAgent:
    "You are an account support agent specializing in login issues and security.",
  productAgent:
    "You are a product support agent specializing in features and how-to guides.",
};

async function answer(ctx: Context, { message }: { message: string }) {
  // 1. First, decide if a specialist is needed
  const routingDecision = await ctx.run(
    "Pick specialist",
    async () => llmCall(message, tools),
    { maxRetryAttempts: 3 },
  );

  // 2. No specialist needed? Give a general answer
  if (!routingDecision.toolCalls || routingDecision.toolCalls.length === 0) {
    return routingDecision.text;
  }

  // 3. Get the specialist's name
  const specialist = routingDecision.toolCalls[0].toolName as Specialist;

  // 4. Ask the specialist to answer
  const { text } = await ctx.run(
    `Ask ${specialist}`,
    async () =>
      llmCall([
        { role: "user", content: message },
        { role: "system", content: PROMPTS[specialist] },
      ]),
    { maxRetryAttempts: 3 },
  );

  return text;
}
```

```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/python-patterns/app/routing_to_agent.py?collapse_prequel"} 
router = restate.Service("AgentRouter")

# Our team of AI specialists
SPECIALISTS = {
    "BillingAgent": "Expert in payments, charges, and refunds",
    "AccountAgent": "Expert in login issues and security",
    "ProductAgent": "Expert in features and how-to guides",
}


@router.handler()
async def answer(ctx: restate.Context, question: Question) -> str:
    """Classify request and route to appropriate specialized agent."""

    # 1. First, decide if a specialist is needed
    routing_decision = await ctx.run_typed(
        "Pick specialist",
        llm_call,  # Use your preferred LLM SDK here
        RunOptions(max_attempts=3),
        system="You are a customer service routing system. Choose the appropriate specialist, or respond directly if no specialist is needed.",
        prompt=question.message,
        tools=[tool(name=name, description=desc) for name, desc in SPECIALISTS.items()],
    )

    # 2. No specialist needed? Give a general answer
    if not routing_decision.tool_calls:
        return routing_decision.content

    # 3. Get the specialist's name
    specialist = routing_decision.tool_calls[0].function.name

    # 4. Ask the specialist to answer
    answer = await ctx.run_typed(
        f"Ask {specialist}",
        llm_call,
        RunOptions(max_attempts=3),
        system=f"You are a {SPECIALISTS.get(specialist, 'support')} specialist.",
        prompt=question.message,
    )

    return answer.content
```

</CodeGroup>

In the Restate UI, you can see how the LLM decides to forward the request to the specialized support agents, and how the response is processed:
<img src="/img/ai/patterns/routing_local_agent.png" alt="Dynamic routing based on LLM output - UI"/>

<Accordion title="Run the example">
    <Steps>
        <StartExample/>

        <Step title="Send a request">
        In the UI (`http://localhost:9070`), click on the `answer_question` handler of the `AgentRouter` service to open the playground and send a default request:
        <img src="/img/ai/patterns/routing_local_agent_playground.png" alt="Multi-agent routing - UI"/>
        </Step>

        <Step title="Check the Restate UI">
        In the UI, you can see how the LLM decides to forward the request to the specialized support agents, and how the response is processed:

        <img src="/img/ai/patterns/routing_local_agent.png" alt="Dynamic routing based on LLM output - UI"/>
        </Step>

    </Steps>
</Accordion>

## Routing to remote agents

For more complex scenarios, you might want to scale and deploy agents as independent services that can run in different infrastructures, languages, or AI SDKs.

Restate lets you call remote agents over HTTP while ensuring end-to-end durability and failure recovery.
You can either use request-response calls for synchronous interactions or asynchronous messaging for long-running tasks or background processing.

Have a look at the service communication docs for your SDK to learn more about the APIs: [TypeScript](/develop/ts/service-communication) / [Python](/develop/python/service-communication).

The example below shows how to route requests to remote agents dynamically by using the name of the selected agent as the service name and implementing the agent logic for each specialist agent in their `run` handler.`

<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/typescript-patterns/src/routing-to-agent.ts#here"} 
const tools = {
  billingAgent: tool({
    description: "Expert in payments, charges, and refunds",
    inputSchema: z.object({}),
  }),
  accountAgent: tool({
    description: "Expert in login issues and security",
    inputSchema: z.object({}),
  }),
  productAgent: tool({
    description: "Expert in features and how-to guides",
    inputSchema: z.object({}),
  }),
} as const;
type Specialist = keyof typeof tools;

const PROMPTS = {
  billingAgent:
    "You are a billing support agent specializing in payments, charges, and refunds.",
  accountAgent:
    "You are an account support agent specializing in login issues and security.",
  productAgent:
    "You are a product support agent specializing in features and how-to guides.",
};

async function answer(ctx: Context, { message }: { message: string }) {
  // 1. First, decide if a specialist is needed
  const routingDecision = await ctx.run(
    "Pick specialist",
    async () => llmCall(message, tools),
    { maxRetryAttempts: 3 },
  );

  // 2. No specialist needed? Give a general answer
  if (!routingDecision.toolCalls || routingDecision.toolCalls.length === 0) {
    return routingDecision.text;
  }

  // 3. Get the specialist's name
  const specialist = routingDecision.toolCalls[0].toolName as Specialist;

  // 4. Ask the specialist to answer
  const { text } = await ctx.run(
    `Ask ${specialist}`,
    async () =>
      llmCall([
        { role: "user", content: message },
        { role: "system", content: PROMPTS[specialist] },
      ]),
    { maxRetryAttempts: 3 },
  );

  return text;
}
```

```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/python-patterns/app/routing_to_remote_agent.py?collapse_prequel"} 
remote_agent_router = restate.Service("RemoteAgentRouter")

# Classify the request
SPECIALISTS = {
    "BillingAgent": "Expert in payments, charges, and refunds",
    "AccountAgent": "Expert in login issues and security",
    "ProductAgent": "Expert in features and how-to guides",
}


@remote_agent_router.handler()
async def answer(ctx: restate.Context, question: Question) -> str:
    """Classify request and route to appropriate specialized agent."""

    # 1. First, decide if a specialist is needed
    routing_decision = await ctx.run_typed(
        "Pick specialist",
        llm_call,  # Use your preferred AI SDK here
        RunOptions(max_attempts=3),
        prompt=question.message,
        tools=[tool(name=name, description=desc) for name, desc in SPECIALISTS.items()],
    )

    # 2. No specialist needed? Give a general answer
    if not routing_decision.tool_calls:
        return routing_decision.content

    # 3. Get the specialist's name
    specialist = routing_decision.tool_calls[0].function.name

    # 4. Call the specialist over HTTP
    response = await ctx.generic_call(
        specialist,
        "run",
        arg=question.model_dump_json().encode(),
    )
    return response.decode("utf-8")
```

</CodeGroup>

Each agent is then implemented in its own service. For example, here is the billing agent implementation:
<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/typescript-patterns/src/utils/utils.ts#here"} 
export const billingAgent = restate.service({
  name: "BillingAgent",
  handlers: {
    run: async (ctx: Context, question: string): Promise<string> => {
      const { text } = await ctx.run(
        "account_response",
        async () =>
          llmCall(`You are a billing support specialist.
            Acknowledge the billing issue, explain charges clearly, provide next steps with timeline.
            ${question}`),
        { maxRetryAttempts: 3 },
      );
      return text;
    },
  },
});
```
```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/python-patterns/app/util/util.py#here"} 
billing_agent_svc = restate.Service("BillingAgent")


@billing_agent_svc.handler("run")
async def get_billing_support(ctx: restate.Context, question: Question) -> str | None:
    result = await ctx.run_typed(
        "LLM call",
        llm_call,
        RunOptions(max_attempts=3),
        system=f"""You are a billing support specialist.
        Acknowledge the billing issue, explain charges clearly, provide next steps with timeline.""",
        prompt=question.message,
    )
    return result.content
```
</CodeGroup>


The Restate UI shows how the LLM decides to forward the request to the specialized remote support agents, and shows the execution trace of the nested calls in a single view.
This is useful for debugging and monitoring complex multi-agent workflows.

<img src="/img/ai/patterns/routing_remote_agent.png" alt="Dynamic routing based on LLM output - UI"/>


<Accordion title="Run the example">
    <Steps>
    <StartExample/>
        <Step title="Send a request">
            In the UI (`http://localhost:9070`), click on the `answer_question` handler of the `RemoteAgentRouter` service to open the playground and send a default request:
            <img src="/img/ai/patterns/routing_remote_agent_playground.png" alt="Multi-agent routing - UI"/>
        </Step>

        <Step title="Check the Restate UI">
            In the UI, you can see how the LLM decides to forward the request to the specialized support agents, and the nested execution trace of the remote calls:

            <img src="/img/ai/patterns/routing_remote_agent.png" alt="Dynamic routing based on LLM output - UI"/>
        </Step>
    </Steps>
</Accordion>