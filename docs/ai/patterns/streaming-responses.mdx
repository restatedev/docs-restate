


- streaming responses from the LLM: either intermediate results or token-per-token
- streaming updates back to the UI/user

pub/sub library