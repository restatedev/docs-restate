---
title: "Routing to Tools and Agents"
sidebarTitle: "Routing"
description: "Implement recoverable routing of tasks to tools and agents with Restate."
tags: ["recipe"]
---

import StartExample from "/snippets/ai-guides/start-example.mdx";

Dynamically route requests to appropriate tools and agents based on context and intent. Restate ensures that all routing decisions, tool calls, and agent interactions are durably logged, enabling automatic retries and recovery from failures.

## How does Restate help?
At the core of routing is an agent loop that repeatedly calls the LLM, checks for tool/agent calls, executes them, and feeds the results back to the LLM until a final answer is produced.

### Durable routing decisions
Restate lets you write standard code while making it resilient through durable execution. The key is wrapping LLM calls and tool execution steps with Restate Context actions.

This design means you don’t need any special logic to implement routing: if the LLM decides to forward a request to a tool or agent, the decision is automatically persisted in Restate’s log, and the workflow can be retried or resumed after failures.

### Durable calls to remote agents and tools

For distributed systems, Restate also provides **durable messaging and RPC**, enabling multi-agent systems with resilient communication.
Restate extends Durable Execution with messaging and RPC, making remote agent calls look like normal function calls, while providing end-to-end resilience.

- Agents and tools can run as independent services across different infrastructures (Kubernetes, Vercel, AWS Lambda, etc.), in different languages
- Communication is durable and failure-resistant
- Work is idempotent, avoiding duplicate processing
- The calling agent can suspend while awaiting results
- Reliably schedule delayed or periodic tasks


## Routing to local tools

You can automatically route requests to tools based on LLM outputs. The agent will continue calling the LLM and executing tools until a final answer is returned.

Restate works well with any AI SDK. Wrap the LLM calls in `ctx.run()` to ensure durability, and process tool calls in a loop until the LLM produces a final answer.
The tool calls can be wrapped in `ctx.run()` as well to ensure durability and automatic retries. Or they can implement multi-step workflows with multiple Restate Context actions for more fine-grained recovery.

The way you specify your tools depends on the AI SDK you are using. Here is an example for Typescript's Vercel AI SDK and Python's LiteLLM SDK:

<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/typescript-patterns/src/routing-to-tools.ts#here"} 
// TOOLS
// Define your tools as your AI SDK requires (here Vercel AI SDK)
const tools = {
  fetchServiceStatus: tool({
    description: "Check service status and outages",
    inputSchema: z.void(),
  }),
  queryUserDatabase: tool({
    description: "Get user account and billing info",
    inputSchema: z.void(),
  }),
  createSupportTicket: tool({
    description: "Create support tickets",
    inputSchema: z.object({
      user_id: z.string().describe("User ID creating the ticket"),
      description: z.string().describe("Detailed description of the issue"),
    }),
  }),
};

// AGENT
async function route(ctx: Context, req: { message: string; userId: string }) {
  const messages: ModelMessage[] = [{ role: "user", content: req.message }];

  while (true) {
      // Call the LLM using your favorite AI SDK
    const result = await ctx.run(
      "LLM call",
      async () => llmCall(messages, tools),
      { maxRetryAttempts: 3 },
    );
    messages.push(...result.messages);

    if (result.finishReason !== "tool-calls") return result.text;

    for (const { toolName, toolCallId, input } of result.toolCalls) {
      let output: string;
        // Use ctx.run to ensure durable execution of tool calls
        switch (toolName) {
        case "queryUserDatabase":
          output = await ctx.run(toolName, () => queryUserDb(req.userId));
          break;
        case "fetchServiceStatus":
          output = await ctx.run(toolName, () => fetchServiceStatus());
          break;
        case "createSupportTicket":
          const ticket = input as SupportTicket;
          output = await ctx.run(toolName, () => createTicket(ticket));
          break;
        default:
          output = `Tool not found: ${toolName}`;
      }
      messages.push(toolResult(toolCallId, toolName, output));
    }
  }
}
```

```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/python-patterns/app/routing_to_tool.py?collapse_prequel"} 
tool_router = restate.Service("ToolRouter")

TOOLS = [
    tool("fetch_service_status", "Check service status and outages"),
    tool("query_user_database", "Get user account and billing info"),
    tool(
        "create_support_ticket",
        "Create support tickets",
        SupportTicket.model_json_schema(),
    ),
]


@tool_router.handler()
async def route(ctx: restate.Context, question: Question) -> str:
    """Route to appropriate tool and execute until final answer"""
    messages = [{"role": "user", "content": question.message}]

    while True:
        result = await ctx.run_typed(
            "LLM call",
            llm_call,
            RunOptions(max_attempts=3, type_hint=Message),
            messages=messages,
            tools=TOOLS,
        )
        messages.append(result.dict())

        if not result.tool_calls:
            return result.content

        for tool_call in result.tool_calls:
            fn = tool_call.function
            match fn.name:
                case "query_user_database":
                    result = await ctx.run_typed(
                        fn.name, query_user_db, user_id=question.user_id
                    )
                case "fetch_service_status":
                    result = await ctx.run_typed(fn.name, fetch_service_status)
                case "create_support_ticket":
                    ticket = SupportTicket.model_validate_json(fn.arguments)
                    result = await ctx.run_typed(
                        fn.name, create_support_ticket, ticket=ticket
                    )
                case _:
                    result = f"Tool not found: {fn.name}"

            messages.append(tool_result(tool_call.id, fn.name, result))
```

</CodeGroup>


When you run the example below, you can see how the LLM decides to forward the request to the technical support tools, and how the response is processed:

<img src="/img/ai/patterns/routing_local_tools.png" alt="Dynamic routing based on LLM output - UI"/>

<Accordion title="Run the example">
    <Steps>
        <StartExample/>

        <Step title="Send a request">
        In the UI (`http://localhost:9070`), click on the `route` handler of the `ToolRouter` service to open the playground and send a default request:
        <img src="/img/ai/patterns/routing_local_tools_playground.png" alt="Dynamic routing LLM calls - UI"/>
        </Step>

        <Step title="Check the Restate UI">
        In the UI, you can see how the LLM decides to forward the request to the technical support tools, and how the response is processed:

        <img src="/img/ai/patterns/routing_local_tools.png" alt="Dynamic routing based on LLM output - UI"/>
        </Step>

    </Steps>
</Accordion>

## Routing to remote tools and workflows

Have a look at the [Workflows as Tools](/ai/patterns/workflows-as-tools) page to learn how to route requests to remote tools and workflows over HTTP while ensuring end-to-end durability and failure recovery.

## Routing to local agents

Route requests to specialized agents based on LLM outputs.

Routing to a local agent is similar to routing to local tools, but instead of calling a tool function, you call the LLM again with a specialized prompt for the selected agent.

<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/typescript-patterns/src/routing-to-agent.ts#here"} 
const tools = {
  billing: tool({
    description: "Expert in payments, charges, and refunds",
    inputSchema: z.object({}),
  }),
  account: tool({
    description: "Expert in login issues and security",
    inputSchema: z.object({}),
  }),
  product: tool({
    description: "Expert in features and how-to guides",
    inputSchema: z.object({}),
  }),
} as const;
type Specialist = keyof typeof tools;

const PROMPTS = {
  billing:
    "You are a billing supportagent specializing in payments, charges, and refunds.",
  account:
    "You are an account support agent specializing in login issues and security.",
  product:
    "You are a product support agent specializing in features and how-to guides.",
};

async function answerQuestion(ctx: Context, { message }: { message: string }) {
  // 1. First, decide if a specialist is needed
  const routingDecision = await ctx.run(
    "pick_specialist",
    async () => llmCall(message, tools),
    { maxRetryAttempts: 3 },
  );

  // 2. No specialist needed? Give a general answer
  if (!routingDecision.toolCalls || routingDecision.toolCalls.length === 0) {
    return routingDecision.text;
  }

  // 3. Get the specialist's name
  const specialist = routingDecision.toolCalls[0].toolName as Specialist;

  // 4. Ask the specialist to answer
  const { text } = await ctx.run(
    `ask_${specialist}`,
    async () =>
      llmCall([
        { role: "user", content: message },
        { role: "system", content: PROMPTS[specialist] },
      ]),
    { maxRetryAttempts: 3 },
  );

  return text;
}
```

```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/python-patterns/app/routing_to_agent.py?collapse_prequel"} 
router = restate.Service("AgentRouter")

# Our team of AI specialists
SPECIALISTS = {
    "BillingAgent": "Expert in payments, charges, and refunds",
    "AccountAgent": "Expert in login issues and security",
    "ProductAgent": "Expert in features and how-to guides",
}


@router.handler()
async def answer_question(ctx: restate.Context, question: Question) -> str:
    """Classify request and route to appropriate specialized agent."""

    # 1. First, decide if a specialist is needed
    routing_decision = await ctx.run_typed(
        "Pick specialist",
        llm_call,
        RunOptions(max_attempts=3),  # Retry up to 3 times if needed
        system="You are a customer service routing system. Choose the appropriate specialist, or respond directly if no specialist is needed.",
        prompt=question.message,
        tools=[tool(name=name, description=desc) for name, desc in SPECIALISTS.items()],
    )

    # 2. No specialist needed? Give a general answer
    if not routing_decision.tool_calls:
        return routing_decision.content

    # 3. Get the specialist's name
    specialist = routing_decision.tool_calls[0].function.name

    # 4. Ask the specialist to answer
    answer = await ctx.run_typed(
        f"Ask {specialist}",
        llm_call,
        RunOptions(max_attempts=3),
        system=f"You are a {SPECIALISTS.get(specialist, 'support')} specialist.",
        prompt=question.message,
    )

    return answer.content
```

</CodeGroup>

In the Restate UI, you can see how the LLM decides to forward the request to the specialized support agents, and how the response is processed:
<img src="/img/ai/patterns/routing_local_agent.png" alt="Dynamic routing based on LLM output - UI"/>

<Accordion title="Run the example">
    <Steps>
        <StartExample/>

        <Step title="Send a request">
        In the UI (`http://localhost:9070`), click on the `answer_question` handler of the `AgentRouter` service to open the playground and send a default request:
        <img src="/img/ai/patterns/routing_local_agent_playground.png" alt="Multi-agent routing - UI"/>
        </Step>

        <Step title="Check the Restate UI">
        In the UI, you can see how the LLM decides to forward the request to the specialized support agents, and how the response is processed:

        <img src="/img/ai/patterns/routing_local_agent.png" alt="Dynamic routing based on LLM output - UI"/>
        </Step>

    </Steps>
</Accordion>

## Routing to remote agents

For more complex scenarios, you might want to scale and deploy agents as independent services that can run in different infrastructures, languages, or AI SDKs.

Restate lets you call remote agents over HTTP while ensuring end-to-end durability and failure recovery.
You can either use request-response calls for synchronous interactions or asynchronous messaging for long-running tasks or background processing.

Have a look at the service communication docs for your SDK to learn more about the APIs: [TypeScript](/develop/ts/service-communication) / [Python](/develop/python/service-communication).

The example below shows how to route requests to remote agents dynamically by using the name of the selected agent as the service name and implementing the agent logic for each specialist agent in their `run` handler.`

<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/typescript-patterns/src/routing-to-agent.ts#here"} 
const tools = {
  billing: tool({
    description: "Expert in payments, charges, and refunds",
    inputSchema: z.object({}),
  }),
  account: tool({
    description: "Expert in login issues and security",
    inputSchema: z.object({}),
  }),
  product: tool({
    description: "Expert in features and how-to guides",
    inputSchema: z.object({}),
  }),
} as const;
type Specialist = keyof typeof tools;

const PROMPTS = {
  billing:
    "You are a billing supportagent specializing in payments, charges, and refunds.",
  account:
    "You are an account support agent specializing in login issues and security.",
  product:
    "You are a product support agent specializing in features and how-to guides.",
};

async function answerQuestion(ctx: Context, { message }: { message: string }) {
  // 1. First, decide if a specialist is needed
  const routingDecision = await ctx.run(
    "pick_specialist",
    async () => llmCall(message, tools),
    { maxRetryAttempts: 3 },
  );

  // 2. No specialist needed? Give a general answer
  if (!routingDecision.toolCalls || routingDecision.toolCalls.length === 0) {
    return routingDecision.text;
  }

  // 3. Get the specialist's name
  const specialist = routingDecision.toolCalls[0].toolName as Specialist;

  // 4. Ask the specialist to answer
  const { text } = await ctx.run(
    `ask_${specialist}`,
    async () =>
      llmCall([
        { role: "user", content: message },
        { role: "system", content: PROMPTS[specialist] },
      ]),
    { maxRetryAttempts: 3 },
  );

  return text;
}
```

```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/python-patterns/app/routing_to_remote_agent.py?collapse_prequel"} 
remote_agent_router = restate.Service("RemoteAgentRouter")

# Classify the request
SPECIALISTS = {
    "BillingAgent": "Expert in payments, charges, and refunds",
    "AccountAgent": "Expert in login issues and security",
    "ProductAgent": "Expert in features and how-to guides",
}


@remote_agent_router.handler()
async def answer_question(ctx: restate.Context, question: Question) -> str:
    """Classify request and route to appropriate specialized agent."""

    # 1. First, decide if a specialist is needed
    routing_decision = await ctx.run_typed(
        "Pick specialist",
        llm_call,
        RunOptions(max_attempts=3),  # Retry up to 3 times if needed
        prompt=question.message,
        tools=[tool(name=name, description=desc) for name, desc in SPECIALISTS.items()],
    )

    # 2. No specialist needed? Give a general answer
    if not routing_decision.tool_calls:
        return routing_decision.content

    # 3. Get the specialist's name
    specialist = routing_decision.tool_calls[0].function.name

    # 4. Call the specialist over HTTP
    response = await ctx.generic_call(
        specialist,
        "run",
        arg=question.model_dump_json().encode(),
    )
    return response.decode("utf-8")
```

</CodeGroup>

Each agent is then implemented in its own service. For example, here is the billing agent implementation:
<CodeGroup>
```ts TypeScript {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/typescript-patterns/src/utils/utils.ts#here"} 
export const billingAgent = restate.service({
  name: "BillingAgent",
  handlers: {
    run: async (ctx: restate.Context, question: string): Promise<string> => {
      const { text } = await ctx.run(
        "account_response",
        async () =>
          llmCall(`You are a billing support specialist.
            Acknowledge the billing issue, explain charges clearly, provide next steps with timeline.
            ${question}`),
        { maxRetryAttempts: 3 },
      );
      return text;
    },
  },
});
```
```python Python {"CODE_LOAD::https://raw.githubusercontent.com/restatedev/ai-examples/refs/heads/typescript_patterns/python-patterns/app/util/util.py#here"} 
billing_agent_svc = restate.Service("BillingAgent")


@billing_agent_svc.handler("run")
async def get_billing_support(ctx: restate.Context, question: Question) -> str | None:
    result = await ctx.run_typed(
        "LLM call",
        llm_call,
        RunOptions(max_attempts=3),
        system=f"""You are a billing support specialist.
        Acknowledge the billing issue, explain charges clearly, provide next steps with timeline.""",
        prompt=question.message,
    )
    return result.content
```
</CodeGroup>


The Restate UI shows how the LLM decides to forward the request to the specialized remote support agents, and shows the execution trace of the nested calls in a single view.
This is useful for debugging and monitoring complex multi-agent workflows.

<img src="/img/ai/patterns/routing_remote_agent.png" alt="Dynamic routing based on LLM output - UI"/>


<Accordion title="Run the example">
    <Steps>
    <StartExample/>
        <Step title="Send a request">
            In the UI (`http://localhost:9070`), click on the `answer_question` handler of the `RemoteAgentRouter` service to open the playground and send a default request:
            <img src="/img/ai/patterns/routing_remote_agent_playground.png" alt="Multi-agent routing - UI"/>
        </Step>

        <Step title="Check the Restate UI">
            In the UI, you can see how the LLM decides to forward the request to the specialized support agents, and the nested execution trace of the remote calls:

            <img src="/img/ai/patterns/routing_remote_agent.png" alt="Dynamic routing based on LLM output - UI"/>
        </Step>
    </Steps>
</Accordion>