---
title: "Data Backups"
description: "Backing up and restoring the Restate data store on single nodes"
---


- What is a snapshot?
Snapshot = internal mechanism to exchange state between nodes in a cluster.
You need it whenever add and remove nodes and for fail-over.
The nodes bootstrap automatically from a snapshot.
Snapshot is always done for a specific partition. The node with the latest state for that partition takes the snapshot.
If you take snapshots for different partitions, they are not guaranteed to be aligned in time.
So you can't use them to restore to a specific point in time.

- What is a backup?
A full backup of all the data Restate stores.
It's more meant for users to revert a cluster to a previous point in time.
It involves manually copying over restate-data basically, or taking a snapshot of EBS volume.
Restate does not let you automatcially take periodic backups, but you can write a script to do that, which would look different for each environment.
A defined point in time to which we want to restore.

Depending on the roles of a node, the backup may include different data.

There is currently not a good way to create a backup of all different nodes at the exact same time.
So mainly for single-node deployments.

- When/why do I use a snapshot
To bootstrap a new node in a cluster.
Otherwise the node would need to replay the entire log to catch up.

Snapshots also allow trimming the log.
(If you take a snapshot, the archived LSN will then show the latest node snapshot for that partition.)

When a node comes up, it will look for the latest snapshot and replay the log from that point.

- vs. a backup
- How this relates to upgrades, fail-over, scaling, etc.
- Single node vs. multi-node deployments
- How to create a backup
- How to restore from a backup

- How to manually take a snapshot
- How to set up automatic snapshotting
- How frequently should snapshot?
Trade-off between storage and writes to S3 vs. catching up from the log after scaling/failure
We let you set a number of records you observe for a partition before taking a snapshot.
Estimate based on the average throughput. The records represent the number of commands in the log, not the number of invocations.
We don't do it on a time schedule. But you can implement a cron job which takes a manual snapshot with restatectl.



Message in Restate Server logs: "Processor has applied log entries beyond the log tail. This indicates data-loss in the log!"
For example, when trying to restore a cluster from a backup, where the backup is a set of EBS snapshots of different nodes.
And because those EBS snapshots might not be perfectly aligned in time, one node might have progressed (running the partition processor)
further the other one (with the log server for that partition).
In that case, you can have a bit of data loss.
This case can happen only in the case where:
- your persistent volumes are corrupt or crashed
- your persistent volume is in a region or zone that is unavailable
Tooling to accept this data loss will be provided in the next release.











<Info>
    This page covers backing up individual Restate server instances. For sharing snapshots in a Restate cluster environment, see [Snapshots](/operate/snapshots).
</Info>

The Restate server persists both metadata (such as the details of deployed services, in-flight invocations) and data (e.g., virtual object and workflow state keys) in its data store, which is located in its base directory (by default, the `restate-data` path relative to the startup working directory). Restate is configured to perform write-ahead logging with fsync enabled to ensure that effects are fully persisted before being acknowledged to participating services.

Backing up the full contents of the Restate base directory will ensure that you can recover this state in the event of a server failure. We recommend placing the data directory on fast block storage that supports atomic snapshots, such as Amazon EBS volume snapshots. Alternatively, you can stop the restate-server process, archive the base directory contents, and then restart the process. This ensures that the backup contains an atomic view of the persisted state.

In addition to the data store, you should also make sure you have a back up of the effective Restate server configuration. Be aware that this may be spread across command line arguments, environment variables, and the server configuration file.

### Restoring Backups

To restore from backup, ensure the following:

* Use a Restate server release that is compatible with the version that produced the data store snapshot. See the [Upgrading](upgrading) section.
* Use an equivalent [Restate server configuration](/operate/configuration/server). In particular, ensure that the `cluster-name` and `node-name` attributes match those of the previous Restate server operating on this data store.
* Exclusive access to a data store restored from the most recent atomic snapshot of the previous Restate installation.

<Warning title={"Prevent multiple instances of the same node"}>
    Restate cannot guarantee that it is the only instance of the given node. You must ensure that only one instance of any given Restate node is running when restoring the data store from a backup. Running multiple instances could lead to a "split-brain" scenario where different servers process invocations for the same set of services, causing state divergence.
</Warning>